{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 10 - How to Connect to Different Data Sources and Sinks\n",
    "So far, I've connected my queries to two different data source types, CSV and JSON, but there are mony more types of datas source formats Spark can read/write data from/to, like Paquet, Avro, XML, JDBC database connections, or Hive Tables. \n",
    "\n",
    "My task for today is to investigate the generic structure of Spark connectors which are implemented by the `pyspark.sql.readwriter` module. Since reading and writing data are tasks in every Spark application, maybe I can derive some basic pattern which I can use for re-usablecode templates.\n",
    "\n",
    "## Class pyspark.sql.readwriter.[DataFrameReader](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/readwriter.html#DataFrameReader)\n",
    "\n",
    "Reading data is a data source -> DataFrame transformation. Since there is no input DataFrame for this transformation I cannot apply it on a DataFrame. Instead the data read API is bound to the `SparkSession`, the access path is:\n",
    "\n",
    "`SparkSession.read`\n",
    "\n",
    "As I observered several times before, there are again multiple options in Spark to get the same results. Actually there are three differnt was of how to read or write data to/from external resources.\n",
    "    \n",
    "The first layout version for data reading puts every option in a single function call. Some options have even its own function name like `format()` and `schema()`. All these option functions are `DataFrameReader` -> `DataFrameReader` transformations so they can be sticked together. Only the `load()` function must be the last on in the chain, because it is a `DataFrameReader` -> `DataFrame` transformation. This is the layout version I've used so far, because I was not aware of the other versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .getOrCreate()\n",
    "\n",
    "myOwnCsv = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"count\",StringType(),False)\n",
    "])\n",
    "\n",
    "csvDF = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"path\", \"./data/flight-data/2015-summary.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .schema(myOwnCsv)\\\n",
    "    .load()\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second layout passes all options as arguments into the `load()` function. This is the most generic layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvDF = spark.read\\\n",
    "    .load(\n",
    "        format=\"csv\",\n",
    "        path=\"./data/flight-data/2015-summary.csv\",\n",
    "        schema=myOwnCsv,\n",
    "        header=True\n",
    "    )\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third layout uses format-specific load functions, e.g. `csv()` to load CSV files. This is the most concise layout and my favourite one from now on. \n",
    "\n",
    "pyspark provides the following load functions out-of the box\n",
    "* `csv()`\n",
    "* `jdbc()`\n",
    "* `json()`\n",
    "* `paquet()`\n",
    "* `orc()`\n",
    "* `table()`\n",
    "* `text()`\n",
    "\n",
    "By the way: *Paquet* is the default format in Spark, because it is column-orientated, which supports column compression and splittable. So if I don't specify the `format` option, Spark or pyspark will take the Parquet format for both read and write operations.\n",
    "\n",
    "Each data source format has its own subset of available options, so I have to reference the pyspark documentation to check, which options are applicable, optional or mandatory. Nonetheless, one generic pattern is that for all file based formats I must define the `path` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvDF = spark.read\\\n",
    "    .csv(\n",
    "        path=\"./data/flight-data/2015-summary.csv\",\n",
    "        header=True,\n",
    "        schema=myOwnCsv\n",
    "    )\n",
    "\n",
    "csvDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class pyspark.sql.readwriter.[DataFrameWriter](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/readwriter.html#DataFrameWriter)\n",
    "\n",
    "Writing data is a DataFrame -> data sink transformation. Therfore the data write API is bound to the `DataFrame`, the access path is:\n",
    "\n",
    "`DataFrame.write`\n",
    "\n",
    "The layout variants are nearly the same as for the reading API except for `load()` is replaced by save functions. \n",
    "pyspark provides the following format specific save functions out-of the box:\n",
    "* `csv()`\n",
    "* `jdbc()`\n",
    "* `json()`\n",
    "* `paquet()`\n",
    "* `orc()`\n",
    "* `saveAsTable()`or `Ã¬nsertInto()`\n",
    "* `text()`\n",
    "    \n",
    "A further generic write parameter is the *save mode* which specifies the behavior of the save operation when data already exists.\n",
    "* `append`: Append contents of this DataFrame to existing data.\n",
    "* `overwrite`: Overwrite existing data.\n",
    "* `ignore`: Silently ignore this operation if data already exists.\n",
    "* `error` or `errorifexists` (default case): Throw an exception if data already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF.write \\\n",
    "    .csv(\n",
    "        path=\"./data/flight-data/2015-output.csv\",\n",
    "        header=True,\n",
    "        sep=\";\",\n",
    "        mode=\"overwrite\",\n",
    "        encoding=\"utf-8\",\n",
    "        compression=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will create a folder \"2015-output.csv\" which represents the DataFrame and store one csv file and one crc checksum file. Values are separated by semicolons and existing files will be overwritten.\n",
    "\n",
    "I can speed up the write operation as well as later read operations, if I partition the data, so Spark will create one file for each partition instead of one big file. Since each file can only be processed by one process at a time, splitting-up the data is pre-requesite for parallel processing.\n",
    "\n",
    "If I want to partition my flight data by destination country, all I have to do is to partition the DataFrame by that column before writing the data to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF \\\n",
    "    .write \\\n",
    "    .partitionBy(\"DEST_COUNTRY_NAME\") \\\n",
    "    .csv(\n",
    "        path=\"./data/flight-data/2015-output.csv\",\n",
    "        header=True,\n",
    "        sep=\";\",\n",
    "        mode=\"overwrite\",\n",
    "        encoding=\"utf-8\",\n",
    "        compression=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will create a folder \"2015-output.csv\" which represents the DataFrame a sub-folder for each partition.\n",
    "\n",
    "<img src= \"./screenshots/day-010/day-010-partitioned-csv-files.png\">\n",
    "\n",
    "## Reading and Writing JSON Files\n",
    "There is an aspect regarding JSON files, I should keep in mind. By default, pyspark assumes that a source JSON file is actually [newline-delimited JSON](http://jsonlines.org), i.e. the file contains only single line JSON objects but many of them.\n",
    "\n",
    "Example:\n",
    "\n",
    "`{\"ORIGIN_COUNTRY_NAME\":\"Romania\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":15}\n",
    "{\"ORIGIN_COUNTRY_NAME\":\"Croatia\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":1}\n",
    "{\"ORIGIN_COUNTRY_NAME\":\"Ireland\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":344}\n",
    "{\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"Egypt\",\"count\":15}\n",
    "{\"ORIGIN_COUNTRY_NAME\":\"India\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":62}`\n",
    "\n",
    "These simplified version of JSON is optimal for record by record processing. Neveretheless the original JSON format allows files containing one big complex JSON object or an array of nested objects with multiple hierarchy levels.\n",
    "\n",
    "Example:\n",
    "\n",
    "`{\"customers\":[\n",
    "    {\n",
    "        \"cusomerId\":1,\n",
    "        \"address:{\"street\":\"Reeperbahn 2\". \"city\":\"Hamburg\", \"country\":\"Germany\"},\n",
    "        \"date-of-birth\":\"1980-12-17\",\n",
    "        \"names\":{\"currentName\":\"Mayer\", \"givenName\":\"Schmitz\"}\n",
    "    },\n",
    "    {\n",
    "        \"cusomerId\":2,\n",
    "        \"address:{\"street\":\"Aachener Strasse 234\". \"city\":\"Cologne\", \"country\":\"Germany\"},\n",
    "        \"date-of-birth\":\"1978-06-27\",\n",
    "        \"names\":{\"currentName\":\"MÃ¼ller\", \"givenName\":\"\"}\n",
    "    },\n",
    "    ... \n",
    "    ]\n",
    "}`\n",
    "\n",
    "To read those complex JSON files, I need to define the option `multiLine=True` (default is False)\n",
    "\n",
    "For more details I can reference to Spark SQL documentation on \n",
    "[JSON files](https://spark.apache.org/docs/latest/sql-data-sources-json.html)\n",
    "\n",
    "## Reading and Writing Paquet Files\n",
    "Paquet files include their own schema definition and enforcement, so when I write data to a Paquet file, I won't have any schema option. Reading from Paquet file always implies schema inference. The only schema related option I have here is to set `mergeSchema=True` to merge schemas collected from Parquet part-files having divergent schemas.\n",
    "\n",
    "For more details I can reference to Spark SQL documentation on \n",
    "[Parquet files](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)\n",
    "\n",
    "## Reading and Writing Avro Files\n",
    "Avro files are supported since Spark 2.4 by an external package `spark-avro`. There is no Avro specific save or load function like `avro()` yet. Instead, I have to specify the datas source format option .format(\"avro\") or .option(format=\"avro\").\n",
    "\n",
    "For more details I can reference to Spark SQL documentation on \n",
    "[Avro files](https://spark.apache.org/docs/latest/sql-data-sources-avro.html)\n",
    "## JDBC Connections to Databases\n",
    "The load and saving options for JDBC database connections are quite different from file formats. Instead of a path and filename, I have to specifie at least:\n",
    "\n",
    "* the JDBC **driver**\n",
    "* the JDBC connection **url**\n",
    "* the database table **dbtable** I want to read or write data\n",
    "\n",
    "`jdbcDF.write.jdbc(\n",
    "    driver=\"postgresql-9.4.1297.jar\",\n",
    "    url=\"jdbc:postgresql://localhost/test?user=fred&password=secret\",\n",
    "    dbtable=\"schema.table_name\"`\n",
    "\n",
    "`jdbcDF.write\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql:dbserver\")\n",
    "  .option(\"dbtable\", \"schema.tablename\")\n",
    "  .option(\"user\", \"username\")\n",
    "  .option(\"password\", \"password\")\n",
    "  .save()`\n",
    "  \n",
    "As I've learned on day 3 that Spark tries to optimize query exection by pushing down predicates to the sources as much as possible. For databases sometimes I can get even better performance results by specifing a **query** instead of a **dbtable** option. Here I can use the entire native SQL dialect of the database I'm connecting to.\n",
    "\n",
    "Another important (even though not madatory) option is **numPartitions**. Spark is designed for highly parallelised distributed data processing. Therefore I have to limit the number of parallel JDBC connections to a degree, the RDBMS can handle it. In conjunction with the **numPartitions** option, I can parallelise table read processes by partitioning the data so each process reads a disjunctive part of the data, by adding the following options.\n",
    "\n",
    "`.option(\"numPartitions\", number)\n",
    ".option(\"partitionColumn\", column)\n",
    ".option(\"lowerBound\", value)\n",
    ".option(\"upperBound\", value)`\n",
    "\n",
    "This *read* partitioning is independent from the physical *table* partitioning in the database. However I will gain the best performance, if my read partitioning goes along with the table partitioning, otherwise there will be a performance bootleneck on the database server.\n",
    "\n",
    "For more details I can reference to Spark SQL documentation on \n",
    "[JDBC connections](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)\n",
    "## Reading and Writing Hive Tables\n",
    "So far I've never worked with Hive tables, so I leave just the link to the Spark SQL documentation on \n",
    "[Hive tables](https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html) for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
