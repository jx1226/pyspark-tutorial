{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview on the pyspark.sql Module\n",
    "During day 2 and 3, I had to use many differnt import statements to to get all functions available for my code examples. So most of the time, I'm asking myself the following questions: \n",
    "* What do I need to import and where do I find that stuff?\n",
    "* What functions are available at all, where can I apply them and which arguments do they expect from me?\n",
    "\n",
    "My goal for today is to get an overview of the *pyspark.sql* module, which is the most relevant module when working with `DataFrames` or SQL in Spark application written in Python. The best source to get an answers to all my questions is the official [pyspark.sql documentation](https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html).\n",
    "\n",
    "For each class or sub-module I will just give an extract of all available methods and attributes, listing only those I 've come across already or which are very representitve for typical operations on DataFrames. \n",
    "\n",
    "##  Class: pyspark.sql.session.[SparkSession](https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#pyspark.sql.SparkSession)\n",
    "Usually the SparkSession object is assigned to the variable named *spark*. Up to know I had three main use cases for this class:\n",
    "* initiating and configuring a Spark session and \n",
    "* accessing a DataFrameReader or DataStreamReader\n",
    "* running SQL query on a table\n",
    "\n",
    "### Embedded Builder Class\n",
    "* SparkSession.**builder.config()**\n",
    "* SparkSession.**builder.getOrCreate()** - cunstructor\n",
    "\n",
    "### Object Properties:\n",
    "* **catalog** - to access the the `Catalog` interface for maintaining metadata regarding databases, tables, functions, etc\n",
    "* **conf** - to access the runtime configuration interface for Spark: `pyspark.sql.conf.RuntimeConfig` \n",
    "* **range()** - to create a DataFrame with single pyspark.sql.types.LongType column named id, containing elements in a range from start to end (exclusive) with step value step.\n",
    "* **read** - to access the `DataFrameReader` interface for reading from data source into a DataFrame\n",
    "* **readStream** - to access the `DataStreamReader` object for reading from data source into a Stream\n",
    "* **sparkContext** - returns the underlying `SparkContext` object\n",
    "\n",
    "### Object Methods:\n",
    "* **createDataFrame()** - to create a DataFrame from an RDD, a list or a pandas.DataFrame\n",
    "* **sql()** - Returns a DataFrame representing the result of the given query\n",
    "\n",
    "### Getting the current settings for a SparkSession\n",
    "The Spark context and session configuration is not part of the Structured SQL API. To get these information I need to access the low-level API.\n",
    "\n",
    "#### Class: pyspark.context.SparkContext\n",
    "A SparkContext represents the connection to a Spark cluster, and can be used to create RDD and broadcast variables on that cluster\n",
    "* **getConf()** - returns the SparkConf object\n",
    "\n",
    "#### Class: pyspark.conf.SparkConf\n",
    "Configuration for a Spark application. Used to set various Spark parameters as key-value pairs.\n",
    "\n",
    "* **get()** - gets the configured value for a given key, or return a default otherwise\n",
    "* **getAll()** - gets all values as a list of key-value pairs\n",
    "* **set()** - sets the configured value for a given key\n",
    "\n",
    "So I can get the current configuration settings of a SparkSession by the following line of code:\n",
    "\n",
    "`spark.sparkContext.getConf().getAll()`\n",
    "\n",
    "## Class: pyspark.sql.dataframe.[DataFrame](https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "Instances of this class are distributed collections of data *Rows* grouped into named *Columns*. A `DataFrame` object is equivalent to a relational table in Spark SQL. In generell, whenever I want to define or change the scope of the data, I want to process, I should take a look at this section of the documentation.\n",
    "\n",
    "### Object Properties:\n",
    "* **columns** - returns all the records as a list of Row\n",
    "* **na** - returns a `DataFrameNaFunctions` object for handling missing values\n",
    "* **rdd** - returns the content as an pyspark.RDD of Row\n",
    "* **schema** - returns the schema of this DataFrame as a pyspark.sql.types.StructType\n",
    "* **stat** - returns a `DataFrameStatFunctions` object for statistic functions\n",
    "* **write** - to access the `DataFrameWriter` interface for writing from a DataFrame to a data sink\n",
    "* **writeStream** - to access the `DataStreamWriter` object for writing Stream data to external storage.\n",
    "\n",
    "### Object Methods:\n",
    "* **alias()** - Returns a new DataFrame with an alias set; similar to table alias in SQL\n",
    "* **count()** - returns the number of rows in this DataFrame\n",
    "* **createOrReplaceTempView()** - Creates or replaces a local temporary view with this DataFrame\n",
    "* **collect()** - returns all the records as a list of Row.\n",
    "* **describe()** - computes basic statistics for numeric and string columns\n",
    "* **distinct()** - returns a new DataFrame containing the distinct rows in this DataFrame\n",
    "* **drop()** - Rrturns a new DataFrame that drops the specified column\n",
    "* **explain()** - returns the physical plan of a transformation, which generates a DataFrame\n",
    "* **filter()** - filters rows using the given condition\n",
    "* **forEach(*f*)** - applies the f function to all Row of this DataFrame; like a map() function in Python\n",
    "* **first** - returns the first row as a Row\n",
    "* **intersect()** - returns a new DataFrame containing rows only in both this DataFrame and another DataFrame\n",
    "* **join()** - joins with another DataFrame, using the given join expression\n",
    "* **limit()** - limits the result count to the number specified\n",
    "* **orderBy()** - returns a new DataFrame sorted by the specified column(s)\n",
    "* **printSchema()** - prints out the schema in the tree format to the console\n",
    "* **randomSplit()** - randomly splits this DataFrame with the provided weights\n",
    "* **repartition()** - returns a new DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is hash partitioned\n",
    "* **sample()** - returns a sampled subset of this DataFrame\n",
    "* **select()** - projects a set of expressions and returns a new DataFrame\n",
    "* **selectExpr()** - projects a set of SQL expressions and returns a new DataFrame; this is a variant of select() that accepts SQL expressions\n",
    "* **show()** - prints the first n rows to the console\n",
    "* **summary()** - computes specified statistics for numeric and string columns. Available statistics are: - count - mean - stddev - min - max - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n",
    "* **sort()** - rrturns a new DataFrame sorted by the specified column(s)\n",
    "* **take()** - returns the first num rows as a list of Row\n",
    "* **toDF()** - returns a new class:DataFrame that with new specified column names\n",
    "* **toJSON()** - Converts a DataFrame into a RDD of string where each string is a JSON document\n",
    "* **toPandas()** - returns the contents of this DataFrame as Pandas pandas.DataFrame\n",
    "- **union()** - returns a new DataFrame containing union of rows in this and another DataFrame\n",
    "* **withColumn()** - returns a new DataFrame by adding a column or replacing the existing column that has the same name\n",
    "* **withColumnRenamed()** - returns a new DataFrame by renaming an existing column\n",
    "* **where()** - where() is an alias for filter()\n",
    "\n",
    "## Class: pyspark.sql.dataframe.[DataFrameStatFunctions](https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#pyspark.sql.DataFrameStatFunctions)\n",
    "Provides methods for statistics functionality with DataFrame.\n",
    "\n",
    "Objects of this class get accessed through the `DataFrame.stat` property so function calls look like this:\n",
    "\n",
    "* `df.stat.corr()`\n",
    "* `df.stat.cov()`\n",
    "\n",
    "Many of the object functions are an alias of corresponding `DataFrame` object methods.\n",
    "\n",
    "##  Class: pyspark.sql.dataframe.[DataFrameNaFunctions](https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions)\n",
    "Methods for handling missing data (null values)\n",
    "\n",
    "Objects of this class get accessed through the `DataFrame.na` property so function calls look like this:\n",
    "\n",
    "* `df.na.fill()`\n",
    "* `df.na.replace()`\n",
    "\n",
    "These methods are aliases of the corresponding `DataFrame` object methods.\n",
    "\n",
    "## Class: pyspark.sql.readwriter.[DataFrameReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)\n",
    "Objects of this class come into place whenever I want to read data from source into a DataFrame. \n",
    "\n",
    "Accessed through the SparkSession object property `spark.read`\n",
    "* spark.read.**option()** - to add an input option for the underlying data source\n",
    "* spark.read.**format()** - to specify the input data source format\n",
    "* spark.read.**schema()** - to specifiy the schema of the input data\n",
    "* spark.read.**csv()** - to load a CSV file; returns the result as a DataFrame\n",
    "* spark.read.**json()** - to load a JSON file; returns the result as a DataFrame\n",
    "* spark.read.**load()** - to load data from datasource; returns the result as a DataFrame \n",
    "\n",
    "## Class: pyspark.sql.readwriter.DataFrameWriter\n",
    "Interface used to write a `DataFrame` to external storage systems (e.g. file systems, key-value stores, etc). Accessed through the `DataFrame.write` property\n",
    "\n",
    "## Class: pyspark.sql.[Column](https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#pyspark.sql.Column)\n",
    "Column instances can be created by expressions, e.g. \n",
    "\n",
    "`df.select(\n",
    "    col(\"ID\") * 100, \n",
    "    lit(23).alias(\"literal\"), \n",
    "    \"Message\"\n",
    "    )`\n",
    "\n",
    "> **Note:**\n",
    "> The third expression `\"Message\"` is a shortcut for `col(\"Message\")`. I can use this shortcut only when I want to pass this column one-by-one from input `DataFrame` to the output `DataFrame`. As soon as I want to apply a column object method, like `alias()` or other operations like `* 100`, I have to use `col(\"Message\")`.  \n",
    "\n",
    "### Object Methods:\n",
    "* **alias()** - returns this column aliased with a new name or names \n",
    "* **asc()** - returns a sort expression based on ascending order of the column\n",
    "* **astype()** - is an alias for cast()\n",
    "* **between()** - a boolean expression that is evaluated to true if the value of this expression is between the given columns\n",
    "* **cast()** - convert the column into the given dataType\n",
    "* **contains()** - returns a boolean Column based on a string match\n",
    "* **desc()** - returns a sort expression based on the descending order of the column\n",
    "* **endswith()** - string ends with. Returns a boolean Column based on a string match\n",
    "* **isin()** - a boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments\n",
    "* **isNotNull()** - true if the current expression is NOT null\n",
    "* **isNull()** - true if the current expression is null\n",
    "* **like()** - SQL like expression. Returns a boolean Column based on a SQL LIKE match\n",
    "* **startswith** - String starts with. Returns a boolean Column based on a string match\n",
    "* **substr()** - return a Column which is a substring of the column\n",
    "* **when()** - evaluates a list of conditions and returns one of multiple possible result expressions. If Column.otherwise() is not invoked, None is returned for unmatched conditions\n",
    "\n",
    "## Module: pyspark.sql.[types](https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#module-pyspark.sql.types)\n",
    "This module provides a collections of Spark data type classes having type specific object functions. Whenever I want to use a type of this module, I need to import it, e.g.:\n",
    "\n",
    "`from pyspark.sql.types import LongType, StringType, BooleanType`\n",
    "\n",
    "`pyspark.sql.types.DataType` is the base class for all data types.\n",
    "\n",
    "Some of the data types are composed of other types, e.g. `StructType` objects consist of a list of `StructField` objects.\n",
    "\n",
    "\n",
    "## Class: pyspark.sql.types.[Row](https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#pyspark.sql.Row)\n",
    "A tupel of data values. Row objects having the same schema can be compiled into a DataFrame object. The data elements can be accessed:\n",
    "* like object attributes: `row.key`\n",
    "* like dictionary values: `row[key]`\n",
    "\n",
    "### Object Methods:\n",
    "* **Row()** - constructor creating Row objects\n",
    "* **asDict()** - returns the data values as an dictionary\n",
    "\n",
    "## Class pyspark.sql.group.[GroupedData](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData)\n",
    "A set of methods for aggregations on a DataFrame, created by `DataFrame.groupBy()`\n",
    "\n",
    "### Object Methods:\n",
    "* **agg()** - compute aggregates and returns the result as a DataFrame\n",
    "* **avg()** - computes average values for each numeric columns for each group\n",
    "* **count()** - counts the number of **records** (not values) for each group\n",
    "* **max()** - computes the max value for each numeric columns for each group\n",
    "* **mean()** - is an alias of **avg()**\n",
    "* **min()** - computes the min value for each numeric columns for each group\n",
    "* **pivot()** - pivots a column of the current DataFrame and performs the specified aggregations\n",
    "* **sum()** - compute the sum for each numeric columns for each group\n",
    "\n",
    "## Module: pyspark.sql.[functions](https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "This module provides a collections of builtin functions available for builing expressions on DataFrames. These functions are similar to those SQL functions I could use \n",
    "* as column expression in the SELECT clause\n",
    "* as join expressions in JOIN ... ON clauses\n",
    "* as filter expressions in the WHERE clause\n",
    "* as sorting expression in ORDER BY clauses, e.g.: ORDER BY name desc;\n",
    "\n",
    "Whenever I want to use a function of this module, I need to import it:\n",
    "\n",
    "`from pyspark.sql.functions import` *funcname*\n",
    "\n",
    "This is just a summary of those functions I've used so far. There are many many more. I will expore them in the next days.\n",
    "* **asc()** - returns a sort expression based on ascending order of the column\n",
    "* **col()** - returns a Column based on the given column name\n",
    "* **count()** - aggregates function: returns the number of items in a group\n",
    "* **countDistinct()** - aggregates function: returns the number of distinct items in a group\n",
    "* **desc()** - returns a sort expression based on descending order of the column\n",
    "* **expr()** - parses the expression string into the column that it represents\n",
    "* **first()** - aggregate function: returns the first value in a group\n",
    "* **instr()** - locates the position of the first occurrence of substr column in the given string; returns null if either of the arguments are null.\n",
    "* **lit()** - creates a Column of literal value\n",
    "* **max()** - aggregate function: returns the maximum value of the expression in a group\n",
    "* **min()** - aggregate function: returns the minimu value of the expression in a group\n",
    "* **sum()** - aggregate function: returns the sum of all values in the expression\n",
    "* **when()** - evaluates a list of conditions and returns one of multiple possible result expressions\n",
    "* **window()** bucketizes rows into one or more time windows given a **timestamp** specifying column\n",
    "\n",
    "**Note:** There are two versions of some functions, like `asc()`, `desc()`,  etc. functions, The first versions are methods of `Column` objects, the second one is an abstract function. so the first one is bound to an object instance and has no arguments, the second gets the column name as argument.\n",
    "* `df.colname.asc()`\n",
    "* `df.sort(asc(\"colname\"))`\n",
    "\n",
    "The same applies to `count()` and `first()` being object methods of `DataFrame` and being aggregate functions `count(\"colname\")` and `first(\"colname\")` of this sub-module.\n",
    "\n",
    "The same applies to `sum()`, `min()`, `max()`, etc. being object methods of `GroupedData` and the aggregate functions having the same names.\n",
    "\n",
    "### Object Methods\n",
    "* **drop()** - returns a new DataFrame omitting rows with null values; is an alias of DataFrame.drop()\n",
    "* **fill()** - Replace null values; is an alias of DataFrame.fillna()\n",
    "* **replace()** - Returns a new DataFrame replacing a value with another value; is an alias of DataFrame.replace()\n",
    "\n",
    "In the next days, I will explore some use cases to get more familiar with a set of API functions, which might be helpful for me to delevop data pipelines and analkysis queries.\n",
    "\n",
    "##  Module: pyspark.sql.[window](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window)\n",
    "\n",
    "### Abstract Class: pyspark.sql.window.Window\n",
    "Provides static utility functions for defining window (WindowSpec objects) in DataFrames.\n",
    "\n",
    "Static functions:\n",
    "* **orderBy()** - creates a WindowSpec with the ordering on defined columns\n",
    "* **partitionBy()** - creates a WindowSpec with the partitioning by defined columns\n",
    "* **rangeBetween(start, end)** - creates a WindowSpec with the frame boundaries defined, from start value (inclusive) to end value (inclusive) both relative from the current row value; **integer** range-based boundaries are based on the actual value of the ORDER BY expression\n",
    "* **rowsBetween(start, end)** - creates a WindowSpec with the frame boundaries defined, from start (inclusive) to end (inclusive) both relative *row positions* from the current row having position = 0\n",
    "\n",
    "### Class: pyspark.sql.window.WindowSpec\n",
    "A window specification that defines the partitioning, ordering, and frame boundaries. Objects should be created by the static methods of the Window class.\n",
    "\n",
    "Object Methods\n",
    "* **orderBy()** - defines the ordering columns in a WindowSpec\n",
    "* **partitionBy()** - defines the partitioning columns in a WindowSpec\n",
    "* **rangeBetween(start, end)** - defines the frame boundaries defined, from start value (inclusive) to end value (inclusive) both relative from the current row value; range-based boundaries are based on the actual value of the ORDER BY expression\n",
    "* **rowsBetween(start, end)** - defines with the frame boundaries defined, rom start (inclusive) to end (inclusive) both relative *row positions* from the current row having position = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}