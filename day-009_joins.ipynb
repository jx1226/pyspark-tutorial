{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 9 - Joins\n",
    "On of the strength of Spark is, that I can combine data from many different sources, regardless of their source format or system. To do this, I need to join data records sharing at least one common key column. So the topic of today is: how does Spark joins data together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I create two DataFrames with students and university data, which I will use to checkout Spark's join operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+-----------+------------+\n",
      "|leftid|       name|date-of-birth|creditscore|universityid|\n",
      "+------+-----------+-------------+-----------+------------+\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|\n",
      "|     4|  April May|   1999-09-30|        110|           1|\n",
      "|     5|Billie Jean|   1990-07-12|        360|        null|\n",
      "+------+-----------+-------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import NullType\n",
    "leftDF = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Peter Pan\", \"1999-04-23\", 90, 3),\n",
    "        (2, \"Alice Smith\", \"2000-05-15\", 100, 2),\n",
    "        (3, \"Bob Miller\", \"2000-09-01\", 80, 3),\n",
    "        (4, \"April May\", \"1999-09-30\", 110, 1),\n",
    "        (5, \"Billie Jean\", \"1990-07-12\", 360, None)\n",
    "    ],\n",
    "    [\"leftid\", \"name\", \"date-of-birth\", \"creditscore\", \"universityid\"]\n",
    ")\n",
    "leftDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+-------+\n",
      "|rightid|       name|        state|founded|\n",
      "+-------+-----------+-------------+-------+\n",
      "|      1|    Harvard|Massachusetts|   1636|\n",
      "|      2|        MIT|Massachusetts|   1861|\n",
      "|      3|   Stanford|   California|   1891|\n",
      "|      4|UC Berkeley|   California|   1868|\n",
      "|      5|  Princeton|   New Jersey|   1746|\n",
      "+-------+-----------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rightDF = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Harvard\", \"Massachusetts\", 1636),\n",
    "        (2, \"MIT\", \"Massachusetts\", 1861),\n",
    "        (3, \"Stanford\", \"California\", 1891),\n",
    "        (4, \"UC Berkeley\", \"California\", 1868),\n",
    "        (5, \"Princeton\", \"New Jersey\", 1746)\n",
    "    ],\n",
    "    [\"rightid\", \"name\", \"state\", \"founded\"]\n",
    ")\n",
    "rightDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Types\n",
    "To perform a join I need to define two things, a join expression and the join type. The join expression can be any complex expressions that evaluates to True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "|leftid|       name|date-of-birth|creditscore|universityid|rightid|    name|        state|founded|\n",
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "|     4|  April May|   1999-09-30|        110|           1|      1| Harvard|Massachusetts|   1636|\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|      3|Stanford|   California|   1891|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|      3|Stanford|   California|   1891|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|      2|     MIT|Massachusetts|   1861|\n",
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinExpression = leftDF[\"universityid\"] == rightDF[\"rightid\"]\n",
    "joinType = \"inner\"\n",
    "\n",
    "leftDF.join(rightDF, on=joinExpression, how=joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "|leftid|       name|date-of-birth|creditscore|universityid|rightid|    name|        state|founded|\n",
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "|     5|Billie Jean|   1990-07-12|        360|        null|   null|    null|         null|   null|\n",
      "|     4|  April May|   1999-09-30|        110|           1|      1| Harvard|Massachusetts|   1636|\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|      3|Stanford|   California|   1891|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|      3|Stanford|   California|   1891|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|      2|     MIT|Massachusetts|   1861|\n",
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_outer\"\n",
    "leftDF.join(rightDF, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+-----------+------------+-------+-----------+-------------+-------+\n",
      "|leftid|       name|date-of-birth|creditscore|universityid|rightid|       name|        state|founded|\n",
      "+------+-----------+-------------+-----------+------------+-------+-----------+-------------+-------+\n",
      "|  null|       null|         null|       null|        null|      5|  Princeton|   New Jersey|   1746|\n",
      "|     4|  April May|   1999-09-30|        110|           1|      1|    Harvard|Massachusetts|   1636|\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|      3|   Stanford|   California|   1891|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|      3|   Stanford|   California|   1891|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|      2|        MIT|Massachusetts|   1861|\n",
      "|  null|       null|         null|       null|        null|      4|UC Berkeley|   California|   1868|\n",
      "+------+-----------+-------------+-----------+------------+-------+-----------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"right_outer\"\n",
    "leftDF.join(rightDF, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+-----------+------------+-------+-----------+-------------+-------+\n",
      "|leftid|       name|date-of-birth|creditscore|universityid|rightid|       name|        state|founded|\n",
      "+------+-----------+-------------+-----------+------------+-------+-----------+-------------+-------+\n",
      "|     5|Billie Jean|   1990-07-12|        360|        null|   null|       null|         null|   null|\n",
      "|  null|       null|         null|       null|        null|      5|  Princeton|   New Jersey|   1746|\n",
      "|     4|  April May|   1999-09-30|        110|           1|      1|    Harvard|Massachusetts|   1636|\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|      3|   Stanford|   California|   1891|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|      3|   Stanford|   California|   1891|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|      2|        MIT|Massachusetts|   1861|\n",
      "|  null|       null|         null|       null|        null|      4|UC Berkeley|   California|   1868|\n",
      "+------+-----------+-------------+-----------+------------+-------+-----------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"full_outer\"\n",
    "leftDF.join(rightDF, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join types:\n",
    "* inner (is the default)\n",
    "* cross \n",
    "* outer, full, full_outer (all are equivalent) \n",
    "* left, left_outer (both are equivalent) \n",
    "* right, right_outer (both are equivalent) \n",
    "* left_semi\n",
    "* left_anti\n",
    "\n",
    "Especially the last to join types are interesting because they implement a do-exist respectively do-not-exist logic. The columns of the right side do not appear in the result set. The right side is just used for the existence check. Using these join types saves me from removing many columns from the result DataFrame if I'm not interested in the column values of the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+-----------+------------+\n",
      "|leftid|       name|date-of-birth|creditscore|universityid|\n",
      "+------+-----------+-------------+-----------+------------+\n",
      "|     4|  April May|   1999-09-30|        110|           1|\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|\n",
      "+------+-----------+-------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_semi\"\n",
    "leftDF.join(rightDF, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+-----------+------------+\n",
      "|leftid|       name|date-of-birth|creditscore|universityid|\n",
      "+------+-----------+-------------+-----------+------------+\n",
      "|     5|Billie Jean|   1990-07-12|        360|        null|\n",
      "+------+-----------+-------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_anti\"\n",
    "leftDF.join(rightDF, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm wondering why there is an additiona `crossJoin()` function, even though I can use `join()` with join type `cross`. So I want to double check if both functions will provide trhe same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "|leftid|       name|date-of-birth|creditscore|universityid|rightid|    name|        state|founded|\n",
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "|     4|  April May|   1999-09-30|        110|           1|      1| Harvard|Massachusetts|   1636|\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|      3|Stanford|   California|   1891|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|      3|Stanford|   California|   1891|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|      2|     MIT|Massachusetts|   1861|\n",
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftDF.join(rightDF, joinExpression, how=\"cross\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks to me, that `join(..., how=\"cross\")` doesn't work because it provides the same result as an inner join. What about `crossJoin()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+-----------+------------+-------+-----------+-------------+-------+\n",
      "|leftid|       name|date-of-birth|creditscore|universityid|rightid|       name|        state|founded|\n",
      "+------+-----------+-------------+-----------+------------+-------+-----------+-------------+-------+\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|      1|    Harvard|Massachusetts|   1636|\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|      2|        MIT|Massachusetts|   1861|\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|      3|   Stanford|   California|   1891|\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|      4|UC Berkeley|   California|   1868|\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|      5|  Princeton|   New Jersey|   1746|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|      1|    Harvard|Massachusetts|   1636|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|      2|        MIT|Massachusetts|   1861|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|      3|   Stanford|   California|   1891|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|      4|UC Berkeley|   California|   1868|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|      5|  Princeton|   New Jersey|   1746|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|      1|    Harvard|Massachusetts|   1636|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|      2|        MIT|Massachusetts|   1861|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|      3|   Stanford|   California|   1891|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|      4|UC Berkeley|   California|   1868|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|      5|  Princeton|   New Jersey|   1746|\n",
      "|     4|  April May|   1999-09-30|        110|           1|      1|    Harvard|Massachusetts|   1636|\n",
      "|     4|  April May|   1999-09-30|        110|           1|      2|        MIT|Massachusetts|   1861|\n",
      "|     4|  April May|   1999-09-30|        110|           1|      3|   Stanford|   California|   1891|\n",
      "|     4|  April May|   1999-09-30|        110|           1|      4|UC Berkeley|   California|   1868|\n",
      "|     4|  April May|   1999-09-30|        110|           1|      5|  Princeton|   New Jersey|   1746|\n",
      "+------+-----------+-------------+-----------+------------+-------+-----------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftDF.crossJoin(rightDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, `crossJoin()` works properly.\n",
    "\n",
    "Just to remind me: I can do all this joining stuff also with my well-known SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "|leftid|       name|date-of-birth|creditscore|universityid|rightid|    name|        state|founded|\n",
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "|     4|  April May|   1999-09-30|        110|           1|      1| Harvard|Massachusetts|   1636|\n",
      "|     1|  Peter Pan|   1999-04-23|         90|           3|      3|Stanford|   California|   1891|\n",
      "|     3| Bob Miller|   2000-09-01|         80|           3|      3|Stanford|   California|   1891|\n",
      "|     2|Alice Smith|   2000-05-15|        100|           2|      2|     MIT|Massachusetts|   1861|\n",
      "+------+-----------+-------------+-----------+------------+-------+--------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftDF.createOrReplaceTempView(\"leftTable\")\n",
    "rightDF.createOrReplaceTempView(\"rightTable\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM leftTable INNER JOIN rightTable ON universityid = rightid\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Joins vs. Broadcast Joins\n",
    "To handle very large data sets, DataFrames get partitioned and these partitions are spread accross the Spark cluster.  Therefore join operations are like to be wide transformations where Spark has to shuffle the data between all nodes to match corresponding rows of the joined DataFrames. As I learned on day 3, shuffle operations are wide transformations which cannot be perfomred entirely in-memory. Additionally, this all-to-all communication between the participating nodes increases heavily with the more nodes get involved. Such *shuffle joins* can cause performance issues in the data pipeline so I would like to avoid them whenever possible.\n",
    "\n",
    "There is actually a chance to avoid a shuffle join when at least one of the joined DataFrames is small enough to fit into each nodes' memory. In that case it can be broadcasted to all nodes so that the join operations can be applied locally and in-memory without further shuffling. To initiate a broadcast join, I hust have to appley the `broadcast()` function on the small table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [universityid#533L], [rightid#825L], Inner, BuildRight\n",
      ":- *(2) Filter isnotnull(universityid#533L)\n",
      ":  +- Scan ExistingRDD[leftid#529L,name#530,date-of-birth#531,creditscore#532L,universityid#533L]\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))\n",
      "   +- *(1) Filter isnotnull(rightid#825L)\n",
      "      +- Scan ExistingRDD[rightid#825L,name#826,state#827,founded#828L]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joinExpression = leftDF[\"universityid\"] == rightDF[\"rightid\"]\n",
    "joinType = \"inner\"\n",
    "\n",
    "leftDF.join(broadcast(rightDF), on=joinExpression, how=joinType).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explain plan shows that two steps of the join transformation: one `BroadcastExchange` step followed by one -> `BroadcastHashJoin` step.\n",
    "    \n",
    "Without using the `broadcast()` function the explain plan shows the shuffle join comprising two parallel `Exchange hashpartitioning` stepts and one final `SortMergeJoin` step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [universityid#533L], [rightid#825L], Inner\n",
      ":- *(2) Sort [universityid#533L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(universityid#533L, 200)\n",
      ":     +- *(1) Filter isnotnull(universityid#533L)\n",
      ":        +- Scan ExistingRDD[leftid#529L,name#530,date-of-birth#531,creditscore#532L,universityid#533L]\n",
      "+- *(4) Sort [rightid#825L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(rightid#825L, 200)\n",
      "      +- *(3) Filter isnotnull(rightid#825L)\n",
      "         +- Scan ExistingRDD[rightid#825L,name#826,state#827,founded#828L]\n"
     ]
    }
   ],
   "source": [
    "leftDF.join(rightDF, on=joinExpression, how=joinType).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}