{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7 - NULL Problems\n",
    "Processing known values is already challenging regarding types, scaling, context and so on. Processing unknown values, NULL values, needs further attention to avoid unexpected results. So today, I want to figure out, how Spark will address this issue.\n",
    "\n",
    "As I know from traditional relational databases, the metadata, i.e. the table schema, defines, if a column is allowed to have empty row cells. When I define al colum as NOT LULL than the RDBMS enforces this by a not-null-contraint and rejects any incoming records violating this rule. \n",
    "\n",
    "I know from day 3, that Spark also knows a schema concept, but chow reliable is it?\n",
    "\n",
    "## Schema Is No Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .getOrCreate()\n",
    "\n",
    "csvData = spark.read\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .format(\"csv\")\\\n",
    "   .load(\"./data/retail-data/by-day/*.csv\")\n",
    "\n",
    "csvData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark schemas have a column nullable attribute. Assumable, Spark observed rows having no value while infering the schema from data samples. Actually I find null values in the CustomerID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   580546|    23406|      CHECK|     -21|2011-12-05 09:27:00|      0.0|      null|United Kingdom|\n",
      "|   580547|    21201|        ???|    -390|2011-12-05 09:29:00|      0.0|      null|United Kingdom|\n",
      "|   580549|   84876B|      found|      66|2011-12-05 09:54:00|      0.0|      null|United Kingdom|\n",
      "|   580561|    22043|     dotcom|      -9|2011-12-05 10:25:00|      0.0|      null|United Kingdom|\n",
      "|   580580|    21804|       null|      10|2011-12-05 10:33:00|      0.0|      null|United Kingdom|\n",
      "|   580586|    21804|     dotcom|       4|2011-12-05 10:34:00|      0.0|      null|United Kingdom|\n",
      "|   580588|    21808|       null|       5|2011-12-05 10:35:00|      0.0|      null|United Kingdom|\n",
      "|  C580604|AMAZONFEE| AMAZON FEE|      -1|2011-12-05 11:35:00|  11586.5|      null|United Kingdom|\n",
      "|  C580605|AMAZONFEE| AMAZON FEE|      -1|2011-12-05 11:36:00| 17836.46|      null|United Kingdom|\n",
      "|   580609|    22927|     Amazon|       1|2011-12-05 11:41:00|      0.0|      null|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvData.where(\"CustomerID is null\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what would happen, when I define a schema explicitly marking the CusomerID column as not nullable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- UnitPrice: integer (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "myOwnCsv = StructType([\n",
    "    StructField(\"InvoiceNo\",StringType(),True),\n",
    "    StructField(\"StockCode\",StringType(),True),\n",
    "    StructField(\"Description\",StringType(),True),\n",
    "    StructField(\"Quantity\",IntegerType(),True),\n",
    "    StructField(\"UnitPrice\",IntegerType(),True),\n",
    "    StructField(\"CustomerID\",DoubleType(),False),\n",
    "    StructField(\"count\",StringType(),True)\n",
    "])\n",
    "\n",
    "csvData2 = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .schema(myOwnCsv)\\\n",
    "    .load(\"./data/day-007/retail-data/by-day/*.csv\")\n",
    "\n",
    "csvData2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to me, that Spark does not care about something like not-null-constraints. even though having defined an explict schema I cannot rely on Spark enforcing it. This can be an issue, especially with regards to key columns. Actually the not-null attribute in the schema is just a hint for the query optimizer.\n",
    "\n",
    "So I have to enforce not-null-constraints it by myself and I have several options to handle this problem.\n",
    "\n",
    "## Option 1: Filtering out Rows Having Null Values\n",
    "This is the most radical solution and I would only use it, if data records missing CustomerID have absolutely no value at all for my purpose so loading them is just wastes storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredData = csvData.where(\"CustomerID is not null\")\n",
    "\n",
    "filteredData.where(\"CustomerID is null\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having datastes with many columns to check for null values, it would anoy me to list douzens of filter rules. The `dropna()` function can make this more convenient. If I need to ensure, that all columns must always have non-null values, I can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropedNA = csvData.dropna(how=\"any\")\n",
    "dropedNA.where(\"CustomerID is null\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I'm less rigid and only whant to prevent empty records, I just need to set `how=all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   580546|    23406|      CHECK|     -21|2011-12-05 09:27:00|      0.0|      null|United Kingdom|\n",
      "|   580547|    21201|        ???|    -390|2011-12-05 09:29:00|      0.0|      null|United Kingdom|\n",
      "|   580549|   84876B|      found|      66|2011-12-05 09:54:00|      0.0|      null|United Kingdom|\n",
      "|   580561|    22043|     dotcom|      -9|2011-12-05 10:25:00|      0.0|      null|United Kingdom|\n",
      "|   580580|    21804|       null|      10|2011-12-05 10:33:00|      0.0|      null|United Kingdom|\n",
      "|   580586|    21804|     dotcom|       4|2011-12-05 10:34:00|      0.0|      null|United Kingdom|\n",
      "|   580588|    21808|       null|       5|2011-12-05 10:35:00|      0.0|      null|United Kingdom|\n",
      "|  C580604|AMAZONFEE| AMAZON FEE|      -1|2011-12-05 11:35:00|  11586.5|      null|United Kingdom|\n",
      "|  C580605|AMAZONFEE| AMAZON FEE|      -1|2011-12-05 11:36:00| 17836.46|      null|United Kingdom|\n",
      "|   580609|    22927|     Amazon|       1|2011-12-05 11:41:00|      0.0|      null|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropedNA = csvData.dropna(how=\"all\")\n",
    "dropedNA.where(\"CustomerID is null\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've also the option to apply this logic only to a subset of columns, where null values are an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   580546|    23406|      CHECK|     -21|2011-12-05 09:27:00|      0.0|      null|United Kingdom|\n",
      "|   580547|    21201|        ???|    -390|2011-12-05 09:29:00|      0.0|      null|United Kingdom|\n",
      "|   580549|   84876B|      found|      66|2011-12-05 09:54:00|      0.0|      null|United Kingdom|\n",
      "|   580561|    22043|     dotcom|      -9|2011-12-05 10:25:00|      0.0|      null|United Kingdom|\n",
      "|   580580|    21804|       null|      10|2011-12-05 10:33:00|      0.0|      null|United Kingdom|\n",
      "|   580586|    21804|     dotcom|       4|2011-12-05 10:34:00|      0.0|      null|United Kingdom|\n",
      "|   580588|    21808|       null|       5|2011-12-05 10:35:00|      0.0|      null|United Kingdom|\n",
      "|  C580604|AMAZONFEE| AMAZON FEE|      -1|2011-12-05 11:35:00|  11586.5|      null|United Kingdom|\n",
      "|  C580605|AMAZONFEE| AMAZON FEE|      -1|2011-12-05 11:36:00| 17836.46|      null|United Kingdom|\n",
      "|   580609|    22927|     Amazon|       1|2011-12-05 11:41:00|      0.0|      null|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropedNA = csvData.dropna(how=\"all\", subset=[\"InvoiceNo\", \"CustomerID\"])\n",
    "dropedNA.where(\"CustomerID is null\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Droping Columns Having Null Values\n",
    "This approach will preserve all data rows but I will completely lose an attribute of my data. If I care about missing values, this column is likely to be relevant for me, otherwise I wouldn't care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "droppedData = csvData.drop(\"CustomerID\")\n",
    "\n",
    "droppedData.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Replacing Nulls by Default Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   580546|    23406|      CHECK|     -21|2011-12-05 09:27:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580547|    21201|        ???|    -390|2011-12-05 09:29:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580549|   84876B|      found|      66|2011-12-05 09:54:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580561|    22043|     dotcom|      -9|2011-12-05 10:25:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580580|    21804|       null|      10|2011-12-05 10:33:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580586|    21804|     dotcom|       4|2011-12-05 10:34:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580588|    21808|       null|       5|2011-12-05 10:35:00|      0.0|      -1.0|United Kingdom|\n",
      "|  C580604|AMAZONFEE| AMAZON FEE|      -1|2011-12-05 11:35:00|  11586.5|      -1.0|United Kingdom|\n",
      "|  C580605|AMAZONFEE| AMAZON FEE|      -1|2011-12-05 11:36:00| 17836.46|      -1.0|United Kingdom|\n",
      "|   580609|    22927|     Amazon|       1|2011-12-05 11:41:00|      0.0|      -1.0|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, coalesce\n",
    "\n",
    "replacedData = csvData.select(\n",
    "    \"InvoiceNo\",\n",
    "    \"StockCode\",\n",
    "    \"Description\",\n",
    "    \"Quantity\",\n",
    "    \"InvoiceDate\",\n",
    "    \"UnitPrice\",\n",
    "    coalesce(col(\"CustomerID\"), lit(-1.0)).alias(\"CustomerID\"),\n",
    "    \"Country\")\n",
    "\n",
    "replacedData.where(col(\"CustomerID\") == -1.0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again Spark provides me an option to apply the same default rule to many columns at once by using `fillna()` instead of `coalesce()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "|   580546|    23406|      CHECK|     -21|2011-12-05 09:27:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580547|    21201|        ???|    -390|2011-12-05 09:29:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580549|   84876B|      found|      66|2011-12-05 09:54:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580561|    22043|     dotcom|      -9|2011-12-05 10:25:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580580|    21804|       null|      10|2011-12-05 10:33:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580586|    21804|     dotcom|       4|2011-12-05 10:34:00|      0.0|      -1.0|United Kingdom|\n",
      "|   580588|    21808|       null|       5|2011-12-05 10:35:00|      0.0|      -1.0|United Kingdom|\n",
      "|  C580604|AMAZONFEE| AMAZON FEE|      -1|2011-12-05 11:35:00|  11586.5|      -1.0|United Kingdom|\n",
      "|  C580605|AMAZONFEE| AMAZON FEE|      -1|2011-12-05 11:36:00| 17836.46|      -1.0|United Kingdom|\n",
      "|   580609|    22927|     Amazon|       1|2011-12-05 11:41:00|      0.0|      -1.0|United Kingdom|\n",
      "+---------+---------+-----------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filledData = csvData.fillna(-1, subset=[\"InvoiceNo\", \"StockCode\", \"CustomerID\"])\n",
    "\n",
    "filledData.where(col(\"CustomerID\") == -1.0).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
