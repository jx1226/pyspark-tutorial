{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: My first Spark Applications and Basic Spark Concepts\n",
    "[pyspark Doc](https://spark.apache.org/docs/2.4.5/api/python/index.html)\n",
    "\n",
    "Spark provides two sets of APIs, *Structured APIs* and *low-level APIs*. The Structured APIs are designed to implement the business logic of Spark applications and they hide the Spark internals of the *low-level API*. So for me, as a ETL developer, the Structured APIs are the best starting point to dive into data processing with Spark.\n",
    "\n",
    "My today's challenge is to write my first little Spark application to get to get a first impression of the *Structured APIs* like `DataFrame`,`Dataset`, *SQL Tables/Views* and *Structured Streaming* and to undertsand some basic concepts like lazy evaluation of transformations, and data processing actions.\n",
    "\n",
    "The first questions, that comes to my mind is, how to start a Spark application?\n",
    "\n",
    "## SparkSession\n",
    "Starting a Spark application generates a Spark job which is controlled and mangaged by exactly one *driver process* and several *executor processes* running across the cluster nodes doing the actual computational work. The driver process is controlled by a`SparkSession` object, which is the entry point of any Spark application, so there is always a one-to-one relationship between SparkSession and Spark application.\n",
    "\n",
    "So how can I start a Spark session?\n",
    "\n",
    "On day 1, I started an interactive Spark session by opening the pyspark console (`./bin/pyspark`). This implicitly creates a `SparkSession` object which is referenced by a variable called *spark*.\n",
    "\n",
    "Since I don't want to type in all the code line by line into the interactive console, my Spark application must create it's own `SparkSession`object. So every Spark application starts with something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark UI\n",
    "Spark provides an UI to monitor the status and progess of Spark jobs. It is available on port 4040 on the driver node in the Spark cluster. Since I'm running Spark in local mode, i.e. all processes are running on my laptop, I can access the UI on http://localhost:4040.\n",
    "\n",
    "After having executed the next code block in the Spark DataFrames section, the UI showed this to me.\n",
    "\n",
    "<img src= \"./screenshots/day-002/day-002_Spark_UI.jpeg\">\n",
    "\n",
    "Clicking on a jobname provides further details and metrics figures regarding the job execution icluding a graphical representation of the execution DAG (directed acyclic graph).\n",
    "\n",
    "<img src= \"./screenshots/day-002/day-002_Spark_UI_job_details.jpeg\">\n",
    "\n",
    "\n",
    "## Spark DataFrames\n",
    "Now I can run my hello world example from day 1, which creates my first `DataFrame`, and similar to the interactive console, the starting point is the `SparkSession` object named *spark*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myFirstDataFrame = spark.range(100).toDF(\"number\")\n",
    "myFirstDataFrame.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark `DataFrame` objects look quite similar to Pandas dataframes. In fact, I can easily transform a Spark `DataFrame` into a Pandas dataframe.\n",
    "\n",
    "***But caution:*** This method should only be used if the resulting Pandas’s DataFrame is expected to be small, as all the data is loaded into the driver’s memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    number\n",
       "0        0\n",
       "1        1\n",
       "2        2\n",
       "3        3\n",
       "4        4\n",
       "..     ...\n",
       "95      95\n",
       "96      96\n",
       "97      97\n",
       "98      98\n",
       "99      99\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasDF = myFirstDataFrame.toPandas()\n",
    "pandasDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, tranforming data to JSON is also easy. Each row is turned into a JSON document as one element in the returned list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"number\":0}', '{\"number\":1}', '{\"number\":2}']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFirstDataFrame.toJSON().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(myFirstDataFrame.toJSON().take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, back to topic. The main difference bewteen Spark and Pandas is, that Pandas dataframes reside on a single machine whereas a Spark `DataFrame`ìs an abstraction of the in-memory optimized low-level API *Resilient Distributed Dataset* (`RDD`), which is designed to be split up data into partitions which can be spread across a cluster of potentially thousends of nodes. \n",
    "Spark `DataFrame`objects have a surprising characteristic, they are immutable once they are created. So how can data processing works when data structures are written in stone?\n",
    "## Lazy Evaluation, Transformations and Actions\n",
    "The few lines of my code already demonstrate that the Structured API has a functional design. Since `DataFrame`objects are immutable, I have to use functions which read `DataFrame` objects as input, do some kind of data transformation and create a new `DataFrame`which again can be the input of another function to do further transformations and generating another `DataFrame`. So finally I can simply concatenate functions to create a sequence of transformations to get my desired data result.\n",
    "\n",
    "Ok, let's do it. I want to see, if how it works. First, I want to read some data from CSV file into a dataframe. The file has a header line and I want Spark to derive the schema, i.e. name and type of the columns, from the file. Nevertheless I could also specify a schema explicitly instead of deriving it from file. Determining the schema processing time, instead of load time, is an example of the common *schema-on-read* design of Big Data architectures.\n",
    "\n",
    "Important to keep in mind is, that the column types are not Python types (or Scala or Java types if I use another API languages). All language API commands are mapped to the Spark internal language *Catalyst* having its own types. That's why all API languages provide the same performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlight = spark.read\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .csv(\"./data/flight-data/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code doesn't show any observable result to me. That looks strange on first view. Actually, Spark hasn't done anything yet, except for deriving the schema by reading a small sample of rows. This is because Spark applies *lazy evaluation* of *transformations*, i.e. no data is moved or processed until Spark is forced to by an *action*, e.g. by calling the `write()` function. \n",
    "\n",
    "By defining transformations, I just give Spark a set of rules describing how a given `Dataframe` should be logically transformed into a new `Dataframe`object. By calling an action, I give Spark the command to apply these transformations, process the data and provide me the results.\n",
    "\n",
    "The reason for the lazy evaluation approach is, that Spark first wants to know the whole story about *what* should be done effectively before it tries to determine an efficient way *how* to do this. Therefore Spark first compiles all transformations into a **logical** directed acyclic graph (DAG), than analyses this DAG, applies optimizations (e.g. predicate push-down to datasources) whenever possible and splits up the optimized **physical** DAG into stages and parallelised tasks of `RDD` manipulations before starting to execute them.\n",
    "\n",
    "Important to note is, tha `DataFrame`objects are kept in memory when ever possible. In contrast to MapReduce, Spark tries to avoid writing intermediate results (i.e. `DataFrame` objects) to disk by *piplining* consecutive in-memory transformations, to gain better performance.\n",
    "\n",
    "This piplining is only possible for *narrow* transformations. These are transformations where each input partition contributes only to one output partition or where the transormation can be applied partion by partition, so the partitions can be processed locally on the same cluster node. Simple row-based filter rules or commutative operations like summing up values, are common examples of narrow transformations.\n",
    "\n",
    "On the other hand, in a *wide* transformation input partitions contribute to multiple output partitions, so data needs to be shuffled across cluster nodes. Sorting and average calculation are common wide transformations across multiple partitions. During shuffling Spark writes results to disk, so wide transformations are not performed in-memory.\n",
    "\n",
    "\n",
    "Ok, now I want to see some action and Spark to show me the first 10 lines in my data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFlight.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I triggered actual data processing so I can see the results. Next to showing data, there are two other types of actions: writing output data, e.g. to file and actions to collect data to native objects in the respercitve language.\n",
    "\n",
    "I can even combine transformations and actions in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .csv(\"./data/flight-data/2015-summary.csv\")\\\n",
    "   .show(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously I can split up any sequence of transformations by asigning the intermedate `Dataframe` object to a variable and have a look into the intermediate results by calling the`show()` function on that `Dataframe`. This is a very nice feature for me when I need to debug complex analytical queries or ETL jobs which compile severeal subqueries together. If my subqueries provide the expected result, the bug must reside in the remaining part of my transormation logic so I can focus my analysis on that area.\n",
    "## Query Explain Plans\n",
    "So I've learned so far, that I just need to define the business logic by concatinating transformation functions and Spark does the optimisation for me. Fortunately Spark gives me insight, how it will perform my query by calling the `explain()` function.\n",
    "\n",
    "I want to see, how Spark would **physically** execute the sorting, which is a wide transformation. Each step in the explain plan actually generate a new `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#30 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#30 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#28,ORIGIN_COUNTRY_NAME#29,count#30] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oli/github/pyspark-tutorial/data/flight-data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "dfFlight.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the explain plan from bottom upwards, it tells me, that first, Spark performs a file scan and than range partitioning is applied shuffling the data over 200 output partitions by default to sort the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm running in local mode on a single machine, it might be better to limit the number of partitions to 5. I can do this be chaging the configuration of the `SparkSession` object *spark*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explain plan confirms, that my configuration change has the desired effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#30 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#30 ASC NULLS FIRST, 5)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#28,ORIGIN_COUNTRY_NAME#29,count#30] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oli/github/pyspark-tutorial/data/flight-data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "dfFlight.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `explain()` function can help me figuring out how flexible I can chain up functions which define `DataFrame` to `DataFrame` transformations. For example, is it relevant for the query execution, whether I filter before selecting or the other way around? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRetail = spark.read\\\n",
    "   .format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .load(\"./data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I get a performance benefit, when I filter very early?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [InvoiceNo#86, Description#88]\n",
      "+- *(1) Filter (isnotnull(InvoiceNo#86) && NOT (cast(InvoiceNo#86 as int) = 536365))\n",
      "   +- *(1) FileScan csv [InvoiceNo#86,Description#88] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oli/github/pyspark-tutorial/data/retail-data/by-day/2011-09-13.csv, ..., PartitionFilters: [], PushedFilters: [IsNotNull(InvoiceNo)], ReadSchema: struct<InvoiceNo:string,Description:string>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfRetail.where(col(\"InvoiceNo\") != 536365)\\\n",
    "    .select(\"InvoiceNo\", \"Description\")\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or can I stick to the well-known SQL pattern: SELECT ... FROM ... WHERE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [InvoiceNo#86, Description#88]\n",
      "+- *(1) Filter (isnotnull(InvoiceNo#86) && NOT (cast(InvoiceNo#86 as int) = 536365))\n",
      "   +- *(1) FileScan csv [InvoiceNo#86,Description#88] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oli/github/pyspark-tutorial/data/retail-data/by-day/2011-09-13.csv, ..., PartitionFilters: [], PushedFilters: [IsNotNull(InvoiceNo)], ReadSchema: struct<InvoiceNo:string,Description:string>\n"
     ]
    }
   ],
   "source": [
    "dfRetail.select(\"InvoiceNo\", \"Description\")\\\n",
    "    .where(col(\"InvoiceNo\") != 536365)\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution plan is the same. Again Spark does the optimization in the background and performs the filter before the column projection. In this case, the functional PAI provides me more flexibility than the strict SQL syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL, Tables and Views\n",
    "<a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html\">SQL Language Reference</a> provided by Databricks.\n",
    "\n",
    "[abc](https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html)\n",
    "\n",
    "I've been working with relational databases and SQL for many years. So I'm happy to notice, that Spark also speaks my languange. In fact, the Spark SQL API supports the ANSI SQL 2003 standard. I can turn a `Dataframe` into a table or view, which I can query with SQL. All I need to do is register a table/view on that `Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlight.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can write my Spark query as I did it for long times in classical databases, and I will get exactly the same result, as doing it the functional way.\n",
    "\n",
    "This example calculates the top 5 countries having the highest number of flight destinations in 2015. Obviously the most flights went to the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# finding the top five destination countries by SQL\n",
    "from pyspark.sql.functions import max, desc\n",
    "# transformation\n",
    "maxSql = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\"\"\")\n",
    "# action\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent functional query looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transformation\n",
    "dfFlight\\\n",
    "   .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "   .sum(\"count\")\\\n",
    "   .withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    "   .sort(desc(\"destination_total\"))\\\n",
    "   .limit(5)\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functional version looks to me even more self-explaing the **logical** transformations. The story reading it line-by-line is: first the data is grouped by the destination countries. Than, for each group (partition) the number of flights are summed up which generates a new, derived column which is than renamed. Afterwards the results are sorted is descending order by the calculated column and the output is limited by the top 5 rows.\n",
    "\n",
    "Fortunately the convenience of using Spark SQL API instead of functions does not have an negative performance impact. The **physical** execution explain plans of both versions are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[aggOrder#108L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#28,destination_total#106L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#28], functions=[sum(cast(count#30 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#28, 5)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#28], functions=[partial_sum(cast(count#30 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#28,count#30] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oli/github/pyspark-tutorial/data/flight-data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "# both transformartions compile to the same plan\n",
    "maxSql.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#151L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#28,destination_total#151L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#28], functions=[sum(cast(count#30 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#28, 5)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#28], functions=[partial_sum(cast(count#30 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#28,count#30] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oli/github/pyspark-tutorial/data/flight-data/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "dfFlight\\\n",
    "   .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "   .sum(\"count\")\\\n",
    "   .withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    "   .sort(desc(\"destination_total\"))\\\n",
    "   .limit(5)\\\n",
    "   .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note is that the `sum()` aggregation involves two *hashAggregate* steps. Because `sum()` is a commutative operation Spark first calculates partial sums partition by partition which is a narrow transformation. Afterwards the aggregted, i.e. already reduced data is shuffled (*Exchange hashpartitioning*) to calculate the overall sum across all partitions. This is another example how Spark optimizes the query execution by first analyzing  all transformations befor starting data processing.\n",
    "## Spark Datasets\n",
    "Datasets are a type-safe version of DataFrames. Since Python is a dynamically taped language, they are not available in pyspark but they can be used in the Java and Scala API. Good to keep in mind, but I skip it for now, since I prefer Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming\n",
    "So far I did all data processing in batch mode, i.e. all data get's processed at once. Batch mode forces me to wait, until all data I want to analyse is available. Stream processing on the other hand enables me to process data incrementally as it arrives, so I can get insights faster.\n",
    "\n",
    "Stream processing in Spark is very similar to data processing in batch mode. The following example will demonstrate this. As far as I can see so for now, this is because Spark stream processing is actually event-triggered mirco-batch-processing. \n",
    "\n",
    "Ok, let's have a closer look and start from scratch, i.e. creating a new `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, column, col, sum\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing\n",
    "This time, my data source is not a single file, instead the data is split into several files on a day-by-day basis. Nevertheless, infering the schema, the meta data, works the same way. The only difference is the wildcard * in the filename to tell Spark, that I want to process all CSV files in the specified folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDF = spark.read\\\n",
    "   .format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .load(\"./data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the transformation which  tells Spark how to create the source DataFrame.\n",
    "\n",
    "As a retailer, I want to analyse how much money each customer is pending in my shops per hour in each 1 day time window.\n",
    "So I add a further transformation on that `DataFrame`, which describes the business logic of my data analysis, and results to another `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = staticDF\\\n",
    "   .selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\")\\\n",
    "   .groupBy(\"CustomerId\", window(\"InvoiceDate\", \"1 day\"))\\\n",
    "   .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take a look at the first 10 rows of the result, I have to call the action `show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   14075.0|[2011-12-05 01:00...|316.78000000000003|\n",
      "|   18180.0|[2011-12-05 01:00...|            310.73|\n",
      "|   15358.0|[2011-12-05 01:00...| 830.0600000000003|\n",
      "|   15392.0|[2011-12-05 01:00...|304.40999999999997|\n",
      "|   15290.0|[2011-12-05 01:00...|263.02000000000004|\n",
      "|   16811.0|[2011-12-05 01:00...|             232.3|\n",
      "|   12748.0|[2011-12-05 01:00...| 363.7899999999999|\n",
      "|   16500.0|[2011-12-05 01:00...| 52.74000000000001|\n",
      "|   16873.0|[2011-12-05 01:00...|1854.8300000000002|\n",
      "|   14060.0|[2011-12-05 01:00...|297.47999999999996|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In batch mode, I'm actually processing the entire data history, i.e. all files at once, which can take quite a long time for large data sets. To make this faster, I can switch to stream processing.\n",
    "### Stream processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are just two things, I have to do, to turn my batch processing into stream processing in Spark:\n",
    " - using `readStream()` instead of `read()`, and\n",
    " - defining a trigger refreshing the result after reading each input file\n",
    "\n",
    "In the given example, a trigger get's fired after reading each file (*maxFilesPerTrigger* = 1). Since all files are already on my harddrive, Spark will actually refresh the results every few (milli-)seconds, so finally I'm quite close to realtime-processing in this demonstration.\n",
    "\n",
    "The schema is the same, as for batch processing, so I'm re-using it from the *staticDF*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDF = spark.readStream\\\n",
    "   .schema(staticDF.schema)\\\n",
    "   .option(\"maxFilesPerTrigger\", 1)\\\n",
    "   .format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .load(\"./data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check, if the Stream creation was successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDF.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation is still the same, but now it is applied on a stream instead of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDF\\\n",
    "   .selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\")\\\n",
    "   .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "   .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I've learnd so far, That Spark evaluates lazly and nothing happens untill I call an action to initiate the stream processing. The action `writeStream` generates a table, which gets updated after each trigger event. Important to note is, **streaming tables are mutable** whereas `DataFrame`objectss **are immutable.**\n",
    "\n",
    "Here I stream the results to my console using `format(\"console\")`, to make it visible how the result table gets updated regularly. Using`format(\"memory\")` would push the stream to an in-memory table so other stream processes could read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7feeda8b5650>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerHour\\\n",
    "   .writeStream\\\n",
    "   .format(\"console\")\\\n",
    "   .queryName(\"customer_purchases\")\\\n",
    "   .outputMode(\"complete\")\\\n",
    "   .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thin, day 2 is completed. My first little Spark applications did their jobs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}