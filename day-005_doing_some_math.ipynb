{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5 - Doing Some Math\n",
    "[pyspark Doc](https://spark.apache.org/docs/2.4.5/api/python/index.html)\n",
    "\n",
    "Doing some calculations is a common task for me when I'm writing ETL jobs, e.g. when amounts need to be aligned to the domestic currency (for me mostly EUR) or when I need to unify the scaling of numeric values. \n",
    "\n",
    "Applying math functions becomes even more impartant to me, when it comes to analytical queries and Key Performance Indicator calculation. So today, I want to have a closer look at the following pyspark sub-modules and classes: \n",
    "* `pyspar.sql.functions`\n",
    "* `pyspark.sql.GroupedData`\n",
    "* `pyspark.sql.DataFrameStatFunctions`\n",
    "\n",
    "## Some Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "   .format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .load(\"./data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the derived schema and a small data sample shows, that Spark interpretes the Customer ID as as decimal numbers (double). Actually they are integers and so I therefore I want to get rid of the decimals. \n",
    "\n",
    "Rounding to zero decimals is not a good option, because the result is still a *double* having a decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+--------------------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|round(CustomerID, 0)|       Country|\n",
      "+---------+---------+--------+---------+--------------------+--------------+\n",
      "|   580538|    23084|      48|     1.79|             14075.0|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|             14075.0|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|             14075.0|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|             14075.0|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|             14075.0|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|             14075.0|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|             14075.0|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69|             14075.0|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|             18180.0|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|             18180.0|United Kingdom|\n",
      "+---------+---------+--------+---------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\", \n",
    "          round(\"CustomerID\",0), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is more an data type issue, rather than a calculation problem, type casting is more appropriate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+----------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------+---------+----------+--------------+\n",
      "|   580538|    23084|      48|     1.79|     14075|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|     14075|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|     14075|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|     14075|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|     14075|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|     14075|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|     14075|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69|     14075|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|     18180|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|     18180|United Kingdom|\n",
      "+---------+---------+--------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\", \n",
    "          col(\"CustomerID\").cast(\"integer\"), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I want to calculate the amount for each invoice position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+------------------+----------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|            Amount|CustomerID|       Country|\n",
      "+---------+---------+--------+---------+------------------+----------+--------------+\n",
      "|   580538|    23084|      48|     1.79|             85.92|     14075|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|              25.0|     14075|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|39.599999999999994|     14075|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|              30.0|     14075|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|15.299999999999999|     14075|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|              40.8|     14075|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|              39.6|     14075|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69|             40.56|     14075|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|              17.0|     18180|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|              17.0|     18180|United Kingdom|\n",
      "+---------+---------+--------+---------+------------------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\",\n",
    "          (col(\"Quantity\") * col(\"UnitPrice\")).alias(\"Amount\"),\n",
    "          col(\"CustomerID\").cast(\"integer\"), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I forgot to round the amount to two decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+------+----------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|Amount|CustomerID|       Country|\n",
      "+---------+---------+--------+---------+------+----------+--------------+\n",
      "|   580538|    23084|      48|     1.79| 85.92|     14075|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|  25.0|     14075|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|  39.6|     14075|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|  30.0|     14075|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|  15.3|     14075|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|  40.8|     14075|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|  39.6|     14075|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69| 40.56|     14075|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "+---------+---------+--------+---------+------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round \n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\",\n",
    "          round((col(\"Quantity\") * col(\"UnitPrice\")), 2).alias(\"Amount\"),\n",
    "          col(\"CustomerID\").cast(\"integer\"), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I save the intermediate results of my data preperation in a variable, to keep the further analytical queries more simple. By decomposing my query into a preperation part and an analytical part, I get the option to check, that the intermediat results are correct and so they a the appropriate foundation of my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "|InvoiceNO|        InvoiceDate|StockCode|Quantity|UnitPrice|Amount|CustomerID|       Country|\n",
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "|   580538|2011-12-05 08:38:00|    23084|      48|     1.79| 85.92|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    23077|      20|     1.25|  25.0|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    22906|      24|     1.65|  39.6|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21914|      24|     1.25|  30.0|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    22467|       6|     2.55|  15.3|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21544|      48|     0.85|  40.8|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    23126|       8|     4.95|  39.6|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21833|      24|     1.69| 40.56|     14075|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    21479|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|   84030E|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf = df.select(\n",
    "    \"InvoiceNO\", \n",
    "    \"InvoiceDate\",\n",
    "    \"StockCode\", \n",
    "    \"Quantity\", \n",
    "    \"UnitPrice\",\n",
    "    round((col(\"Quantity\") * col(\"UnitPrice\")), 2).alias(\"Amount\"),\n",
    "    col(\"CustomerID\").cast(\"integer\"), \n",
    "    \"Country\")\n",
    "\n",
    "preparedDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations\n",
    "### Aggregating on DataFrames\n",
    "First I want to apply several aggregation functions on the entire dataset in the `DataFrame` to do some data profiling. To make the output more readable, I switch `show()` to vertical output to get many rows instead of many columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " count                 | 541909              \n",
      " countDistinct         | 5827                \n",
      " approx_count_distinct | 5417                \n",
      " sum                   | 9747747.929999992   \n",
      " sumDistinct           | 863586.9200000005   \n",
      " min                   | -168469.6           \n",
      " max                   | 168469.6            \n",
      " avg                   | 17.987794869618316  \n",
      " mean                  | 17.987794869618316  \n",
      " variance              | 143497.64000554013  \n",
      " var_samp              | 143497.64000554013  \n",
      " var_pop               | 143497.37520528768  \n",
      " stddev                | 378.81082350632505  \n",
      " kurtosis              | 151196.60137753483  \n",
      " skewness              | -0.9643865070858197 \n",
      " first                 | 3.26                \n",
      " last                  | 176.48              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "preparedDf\\\n",
    "    .select(\n",
    "        count(\"Amount\").alias(\"count\"), \n",
    "        countDistinct(\"Amount\").alias(\"countDistinct\"),\n",
    "        approx_count_distinct(\"Amount\", rsd=0.1).alias(\"approx_count_distinct\"),\n",
    "        sum(\"Amount\").alias(\"sum\"), \n",
    "        sumDistinct(\"Amount\").alias(\"sumDistinct\"),\n",
    "        min(\"Amount\").alias(\"min\"), \n",
    "        max(\"Amount\").alias(\"max\"), \n",
    "        avg(\"Amount\").alias(\"avg\"), \n",
    "        mean(\"Amount\").alias(\"mean\"), \n",
    "        variance(\"Amount\").alias(\"variance\"), \n",
    "        var_samp(\"Amount\").alias(\"var_samp\"),\n",
    "        var_pop(\"Amount\").alias(\"var_pop\"),\n",
    "        stddev(\"Amount\").alias(\"stddev\"),\n",
    "        kurtosis(\"Amount\").alias(\"kurtosis\"),\n",
    "        skewness(\"Amount\").alias(\"skewness\"),\n",
    "        first(\"Amount\").alias(\"first\"),\n",
    "        last(\"Amount\").alias(\"last\")\n",
    "    )\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are further aggregating I don't have a use case in my example here, functions like:\n",
    "* **corr(col1, col2)** - returns a new Column for the Pearson Correlation Coefficient for col1 and col2\n",
    "* **covar_pop(col1, col2)** - returns a new Column for the population covariance of col1 and col2\n",
    "* **covar_samp(col1, col2)** - Returns a new Column for the sample covariance of col1 and col2\n",
    "\n",
    "Most of the aggregation function names are self-explaining, so nothing to comment on. Just first() and last() are a bit special. In contrast to most of the other aggregation functions, `first()` and `last()` both refer to the value **position** in the dataset and not to the value **amount**, like `min()` and `max()` do. So `first()` and `last()` are the only aggregation functions, being affected by data sorting.\n",
    "\n",
    "Back to data profiling. The ratio between count and countDistinct is an important indicator to identify key candidate columns. For primary keys, the ratio must be 1, i.e. countDistinct must equal the total count of values so it's cardinality must be also 1 to ensure uniqueness. \n",
    "\n",
    "Even though not beeing unique, columns with low cardinality are still candidates for performant data acess patterns. The cardinality is a measure of the average number of rows I will get when filtering in such a column value.\n",
    "\n",
    "The reverse value of cardinality, the entropy, is an indicator how compressible a column is. `DataFrames` having many columns with low entropy benefit much from a columnar storage format. On the other extreme, primary key columns are not compressible at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " count         | 541909               \n",
      " countDistinct | 5827                 \n",
      " cardinality   | 92.99965677020765    \n",
      " entropy       | 0.010752727856522036 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .select(\n",
    "        count(\"Amount\").alias(\"count\"), \n",
    "        countDistinct(\"Amount\").alias(\"countDistinct\")\n",
    "    )\\\n",
    "    .withColumn(\"cardinality\", col(\"count\") / col(\"countDistinct\"))\\\n",
    "    .withColumn(\"entropy\", col(\"countdistinct\") / col(\"count\"))\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not suprisingly the amount column is not a key candidate but it is very interesting, that the cardinality is quite high. Maybe there are only a few standard unit prices and/or lot sizes I can put orders on. Let's compare it with the *InvoiceNO* column. The column name sounds like a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " count         | 541909               \n",
      " countDistinct | 25900                \n",
      " cardinality   | 20.923127413127414   \n",
      " entropy       | 0.047794002314041656 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .select(\n",
    "        count(\"InvoiceNo\").alias(\"count\"), \n",
    "        countDistinct(\"InvoiceNo\").alias(\"countDistinct\")\n",
    "    )\\\n",
    "    .withColumn(\"cardinality\", col(\"count\") / col(\"countDistinct\"))\\\n",
    "    .withColumn(\"entropy\", col(\"countdistinct\") / col(\"count\"))\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this column has a much lower cardinality but stii it is not a unique key. The reason is, that the retail dataset is denormalized and the granularity is not based on invoices but on stock items. Since each invoice can list multiple stock items, I need to combine InviceNo and StockCode to get a unique key. Let's check, if this solves my problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "|          Key|InvoiceNO|        InvoiceDate|StockCode|Quantity|UnitPrice|Amount|CustomerID|       Country|\n",
      "+-------------+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "| 580538-23084|   580538|2011-12-05 08:38:00|    23084|      48|     1.79| 85.92|     14075|United Kingdom|\n",
      "| 580538-23077|   580538|2011-12-05 08:38:00|    23077|      20|     1.25|  25.0|     14075|United Kingdom|\n",
      "| 580538-22906|   580538|2011-12-05 08:38:00|    22906|      24|     1.65|  39.6|     14075|United Kingdom|\n",
      "| 580538-21914|   580538|2011-12-05 08:38:00|    21914|      24|     1.25|  30.0|     14075|United Kingdom|\n",
      "| 580538-22467|   580538|2011-12-05 08:38:00|    22467|       6|     2.55|  15.3|     14075|United Kingdom|\n",
      "| 580538-21544|   580538|2011-12-05 08:38:00|    21544|      48|     0.85|  40.8|     14075|United Kingdom|\n",
      "| 580538-23126|   580538|2011-12-05 08:38:00|    23126|       8|     4.95|  39.6|     14075|United Kingdom|\n",
      "| 580538-21833|   580538|2011-12-05 08:38:00|    21833|      24|     1.69| 40.56|     14075|United Kingdom|\n",
      "| 580539-21479|   580539|2011-12-05 08:39:00|    21479|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|580539-84030E|   580539|2011-12-05 08:39:00|   84030E|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "+-------------+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyedDf = preparedDf.select(\n",
    "    concat_ws('-', \"InvoiceNO\",\"StockCode\").alias(\"Key\"),\n",
    "    \"InvoiceNO\", \n",
    "    \"InvoiceDate\",\n",
    "    \"StockCode\", \n",
    "    \"Quantity\", \n",
    "    \"UnitPrice\",\n",
    "    \"Amount\",\n",
    "    \"CustomerID\", \n",
    "    \"Country\")\n",
    "\n",
    "keyedDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " count         | 541909             \n",
      " countDistinct | 531225             \n",
      " cardinality   | 1.0201120052708363 \n",
      " entropy       | 0.9802845127133891 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyedDf\\\n",
    "    .select(\n",
    "        count(\"Key\").alias(\"count\"), \n",
    "        countDistinct(\"Key\").alias(\"countDistinct\")\n",
    "    )\\\n",
    "    .withColumn(\"cardinality\", col(\"count\") / col(\"countDistinct\"))\\\n",
    "    .withColumn(\"entropy\", col(\"countdistinct\") / col(\"count\"))\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mh, I'm getting close but there are still some duplicates. Maybe there are some Null values in these columns. I would need ot investigate it furthr down, but I leave this for now because another phenomenon confuses me, there are two versions of counting in Spark:\n",
    "* `DataFrame.count()`\n",
    "* `pyspark.sql.functions.count()`\n",
    "\n",
    "The `DataFrame.count()` method is always applied on the entire `DataFrame` and counts the total number of physical rows in it. Additionally this method is an action and not a transformation, because the count number is directy determined and returned. On the other hand `pyspark.sql.functions.count()` is an aggregation function counting non-Null values which is applied on grouped data defined by a grouping key `DataFrame.groupBy()`or a window function. Aggregation functions define lazly evaluated transformations. \n",
    "## Aggregating on Grouped Data\n",
    "Aggregating on the entire DataFrame will show me just in row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+--------+------------------+------------------+\n",
      "|              sum|      min|     max|               avg|              mean|\n",
      "+-----------------+---------+--------+------------------+------------------+\n",
      "|9747747.929999745|-168469.6|168469.6|17.987794869617858|17.987794869617858|\n",
      "+-----------------+---------+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .select(\n",
    "        sum(\"Amount\").alias(\"sum\"), \n",
    "        min(\"Amount\").alias(\"min\"), \n",
    "        max(\"Amount\").alias(\"max\"), \n",
    "        avg(\"Amount\").alias(\"avg\"), \n",
    "        mean(\"Amount\").alias(\"mean\")\n",
    "    )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such highly aggregated data does not provide me much business insight, so I want to see the results for each country. So the first thing I have to do is to define a grouping key to arrange the data to get one group for each country. Than I can aggregate on each group seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|           Country|count|\n",
      "+------------------+-----+\n",
      "|            Sweden|  462|\n",
      "|         Singapore|  229|\n",
      "|           Germany| 9495|\n",
      "|               RSA|   58|\n",
      "|            France| 8557|\n",
      "|            Greece|  146|\n",
      "|European Community|   61|\n",
      "|           Belgium| 2069|\n",
      "|           Finland|  695|\n",
      "|             Malta|  127|\n",
      "+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf.groupBy(\"Country\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I already know, rearranging data means, Spark is shuffling partitions around. The explain plan confirms this (*Exchange hashpartitioning*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Country#17], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(Country#17, 200)\n",
      "   +- *(1) HashAggregate(keys=[Country#17], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [Country#17] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oli/github/pyspark-tutorial/data/day-005/retail-data/by-day/2011-09-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Country:string>\n"
     ]
    }
   ],
   "source": [
    "preparedDf.groupBy(\"Country\").count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "|           Country|               sum|     min|    max|               avg|              mean|\n",
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "|            Sweden|36595.909999999996| -1188.0| 1188.0|  79.2119264069264|  79.2119264069264|\n",
      "|         Singapore|           9120.39|-3949.32|3949.32| 39.82703056768559| 39.82703056768559|\n",
      "|           Germany|         221698.21|  -599.5|  876.0|23.348942601369142|23.348942601369142|\n",
      "|               RSA|1002.3099999999998|     0.0|  38.25|17.281206896551723|17.281206896551723|\n",
      "|            France|197403.90000000008|-8322.12|4161.06|   23.069288301975|   23.069288301975|\n",
      "|            Greece|           4710.52|   -50.0|  175.2| 32.26383561643836| 32.26383561643836|\n",
      "|European Community|1291.7500000000002|    -8.5|   60.0|21.176229508196727|21.176229508196727|\n",
      "|           Belgium|          40910.96|  -19.95|  165.0|19.773301111648138|19.773301111648138|\n",
      "|           Finland|22326.739999999994|   -80.0|  551.2| 32.12480575539568| 32.12480575539568|\n",
      "|             Malta|           2505.47|  -130.0|  455.0| 19.72811023622047| 19.72811023622047|\n",
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .groupBy(\"Country\")\\\n",
    "    .agg(\n",
    "        sum(\"Amount\").alias(\"sum\"), \n",
    "        min(\"Amount\").alias(\"min\"), \n",
    "        max(\"Amount\").alias(\"max\"), \n",
    "        avg(\"Amount\").alias(\"avg\"), \n",
    "        mean(\"Amount\").alias(\"mean\")\n",
    "    )\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to remind myself: I could do this all using my good-old SQL. (ref. <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html\">SQL Language Reference</a> provided by Databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "|           Country|               sum|     min|    max|               avg|              mean|\n",
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "|            Sweden|36595.909999999996| -1188.0| 1188.0|  79.2119264069264|  79.2119264069264|\n",
      "|         Singapore|           9120.39|-3949.32|3949.32| 39.82703056768559| 39.82703056768559|\n",
      "|           Germany|         221698.21|  -599.5|  876.0|23.348942601369142|23.348942601369142|\n",
      "|               RSA|1002.3099999999998|     0.0|  38.25|17.281206896551723|17.281206896551723|\n",
      "|            France|197403.90000000008|-8322.12|4161.06|   23.069288301975|   23.069288301975|\n",
      "|            Greece|           4710.52|   -50.0|  175.2| 32.26383561643836| 32.26383561643836|\n",
      "|European Community|1291.7500000000002|    -8.5|   60.0|21.176229508196727|21.176229508196727|\n",
      "|           Belgium|          40910.96|  -19.95|  165.0|19.773301111648138|19.773301111648138|\n",
      "|           Finland|22326.739999999994|   -80.0|  551.2| 32.12480575539568| 32.12480575539568|\n",
      "|             Malta|           2505.47|  -130.0|  455.0| 19.72811023622047| 19.72811023622047|\n",
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf.createOrReplaceTempView(\"retailTable\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Country, sum(Amount) as sum, min(Amount) as min, max(Amount) as max, \n",
    "        avg(Amount) as avg, mean(Amount) as mean \n",
    "    FROM retailTable\n",
    "    GROUP BY Country\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating on Floating Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the average amount per country over the entire data set history of aprox. one year is still very high-level. I would like to make a time-series analysis on the invoice mounts. So the first thing I have to do is to include the *InvoiceDate* column into the grouping key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+\n",
      "|  Country|        InvoiceDate|                avg|\n",
      "+---------+-------------------+-------------------+\n",
      "|Australia|2011-05-20 14:13:00|             191.59|\n",
      "|Australia|2011-07-26 10:15:00|             -38.16|\n",
      "|Australia|2011-09-28 14:26:00|               37.5|\n",
      "|Australia|2011-07-19 12:26:00|-2.7833333333333328|\n",
      "|Australia|2011-07-13 15:30:00| 124.32545454545456|\n",
      "|Australia|2011-09-28 15:41:00|              20.86|\n",
      "|Australia|2011-05-23 09:14:00| 27.769999999999996|\n",
      "|Australia|2011-05-31 11:29:00|               -9.9|\n",
      "|Australia|2011-09-05 09:48:00|            18.4625|\n",
      "|Australia|2011-07-19 10:51:00| 2.7833333333333345|\n",
      "+---------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .groupBy(\"Country\", \"InvoiceDate\")\\\n",
    "    .agg( \n",
    "        avg(\"Amount\").alias(\"avg\")\n",
    "    )\\\n",
    "    .orderBy(\"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *InvoiceDate* is actually a timestamp, now I have to much details because approximaly each row is representig one invoice. To reduce the noice in my time series due to volatile invoice amounts, I would like to calculate rolling 7-day average amounts. Grouping the data does not help me here because this would assign each row to ecactly one group. Now each row should be member of seven overlapping time windows each spanning over seven days.\n",
    "\n",
    "So first, I have to define how to generate 7-day time windows. Than I can aggregate, averaging in particular, the amounts in each window. \n",
    "\n",
    "Ok, how can I define sliding windows? Maybe the sub-module `pyspark.sql.window` can help here?\n",
    "To get sliding time windows for each country, I have to partition the data by that column. Next I have to order the data along the date because the window boundaries are based on it. Finally I define the bounderies based on the date **values** relative to the date of the current row. Since there can be more than one row per day I cannot use rowsBetween() which is **position** based. Instead `rangeBetween()` seems more suitable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "sevenDayWindows = Window\\\n",
    "    .partitionBy(\"Country\")\\\n",
    "    .orderBy(\"InvoiceDate\")\\\n",
    "    .rangeBetween(-7, Window.currentRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can apply my `avg()` aggregation on each timewindow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '(PARTITION BY `Country` ORDER BY `InvoiceDate` ASC NULLS FIRST RANGE BETWEEN -7L FOLLOWING AND CURRENT ROW)' due to data type mismatch: The data type 'timestamp' used in the order specification does not match the data type 'bigint' which is used in the range frame.;;\\n'Project [Country#17, InvoiceDate#14, avg(Amount#197) windowspecdefinition(Country#17, InvoiceDate#14 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -7, currentrow$())) AS 7-day-avg#22450]\\n+- Project [InvoiceNO#10, InvoiceDate#14, StockCode#11, Quantity#13, UnitPrice#15, round((cast(Quantity#13 as double) * UnitPrice#15), 2) AS Amount#197, cast(CustomerID#16 as int) AS CustomerID#198, Country#17]\\n   +- Relation[InvoiceNo#10,StockCode#11,Description#12,Quantity#13,InvoiceDate#14,UnitPrice#15,CustomerID#16,Country#17] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/python/python3-7/pyspark-tutorial/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/python3-7/pyspark-tutorial/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o92.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '(PARTITION BY `Country` ORDER BY `InvoiceDate` ASC NULLS FIRST RANGE BETWEEN -7L FOLLOWING AND CURRENT ROW)' due to data type mismatch: The data type 'timestamp' used in the order specification does not match the data type 'bigint' which is used in the range frame.;;\n'Project [Country#17, InvoiceDate#14, avg(Amount#197) windowspecdefinition(Country#17, InvoiceDate#14 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -7, currentrow$())) AS 7-day-avg#22450]\n+- Project [InvoiceNO#10, InvoiceDate#14, StockCode#11, Quantity#13, UnitPrice#15, round((cast(Quantity#13 as double) * UnitPrice#15), 2) AS Amount#197, cast(CustomerID#16 as int) AS CustomerID#198, Country#17]\n   +- Relation[InvoiceNo#10,StockCode#11,Description#12,Quantity#13,InvoiceDate#14,UnitPrice#15,CustomerID#16,Country#17] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8ef77b1ff545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"InvoiceDate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Amount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msevenDayWindows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"7-day-avg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m ).show(10)\n",
      "\u001b[0;32m~/python/python3-7/pyspark-tutorial/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/python3-7/pyspark-tutorial/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/python3-7/pyspark-tutorial/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '(PARTITION BY `Country` ORDER BY `InvoiceDate` ASC NULLS FIRST RANGE BETWEEN -7L FOLLOWING AND CURRENT ROW)' due to data type mismatch: The data type 'timestamp' used in the order specification does not match the data type 'bigint' which is used in the range frame.;;\\n'Project [Country#17, InvoiceDate#14, avg(Amount#197) windowspecdefinition(Country#17, InvoiceDate#14 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -7, currentrow$())) AS 7-day-avg#22450]\\n+- Project [InvoiceNO#10, InvoiceDate#14, StockCode#11, Quantity#13, UnitPrice#15, round((cast(Quantity#13 as double) * UnitPrice#15), 2) AS Amount#197, cast(CustomerID#16 as int) AS CustomerID#198, Country#17]\\n   +- Relation[InvoiceNo#10,StockCode#11,Description#12,Quantity#13,InvoiceDate#14,UnitPrice#15,CustomerID#16,Country#17] csv\\n\""
     ]
    }
   ],
   "source": [
    "preparedDf.select(\n",
    "    \"Country\",\n",
    "    \"InvoiceDate\",\n",
    "    avg(\"Amount\").over(sevenDayWindows).alias(\"7-day-avg\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ups, what' wrong now? The trace strack states:\n",
    "> data type mismatch: The data type 'date' used in the order specification does not match the data type 'bigint' which is used in the range frame\n",
    "\n",
    "Ok, `Window.rangeBetween()` is obviously only suitable for integer ranges but not for date ranges. Fortunately there is another `window()` function available from sub-module `pyspark.sql.function`. The pyspark  <a href=\"https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#module-pyspark.sql.functions\">documentation</a> says:\n",
    "> pyspark.sql.functions.window(*timeColumn, windowDuration, slideDuration=None, startTime=None*)\n",
    "\n",
    "> Bucketize rows into one or more time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive\n",
    "\n",
    "I want to have 7-day windows sliding every day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+------------------+\n",
      "|Country  |7-day windows                             |avg               |\n",
      "+---------+------------------------------------------+------------------+\n",
      "|Australia|[2010-11-25 01:00:00, 2010-12-02 01:00:00]|25.589285714285715|\n",
      "|Australia|[2010-11-26 01:00:00, 2010-12-03 01:00:00]|25.589285714285715|\n",
      "|Australia|[2010-11-27 01:00:00, 2010-12-04 01:00:00]|25.589285714285715|\n",
      "|Australia|[2010-11-28 01:00:00, 2010-12-05 01:00:00]|25.589285714285715|\n",
      "|Australia|[2010-11-29 01:00:00, 2010-12-06 01:00:00]|25.589285714285715|\n",
      "|Australia|[2010-11-30 01:00:00, 2010-12-07 01:00:00]|25.589285714285715|\n",
      "|Australia|[2010-12-01 01:00:00, 2010-12-08 01:00:00]|25.589285714285715|\n",
      "|Australia|[2010-12-02 01:00:00, 2010-12-09 01:00:00]|32.362500000000004|\n",
      "|Australia|[2010-12-03 01:00:00, 2010-12-10 01:00:00]|32.362500000000004|\n",
      "|Australia|[2010-12-04 01:00:00, 2010-12-11 01:00:00]|32.362500000000004|\n",
      "|Australia|[2010-12-05 01:00:00, 2010-12-12 01:00:00]|32.362500000000004|\n",
      "|Australia|[2010-12-06 01:00:00, 2010-12-13 01:00:00]|32.362500000000004|\n",
      "|Australia|[2010-12-07 01:00:00, 2010-12-14 01:00:00]|32.362500000000004|\n",
      "|Australia|[2010-12-08 01:00:00, 2010-12-15 01:00:00]|21.013636363636365|\n",
      "|Australia|[2010-12-09 01:00:00, 2010-12-16 01:00:00]|-9.25             |\n",
      "|Australia|[2010-12-10 01:00:00, 2010-12-17 01:00:00]|-9.25             |\n",
      "|Australia|[2010-12-11 01:00:00, 2010-12-18 01:00:00]|29.842307692307692|\n",
      "|Australia|[2010-12-12 01:00:00, 2010-12-19 01:00:00]|29.842307692307692|\n",
      "|Australia|[2010-12-13 01:00:00, 2010-12-20 01:00:00]|29.842307692307692|\n",
      "|Australia|[2010-12-14 01:00:00, 2010-12-21 01:00:00]|29.842307692307692|\n",
      "+---------+------------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .groupBy(\n",
    "        \"Country\", \n",
    "        window(timeColumn=\"InvoiceDate\", windowDuration=\"7 days\", slideDuration=\"1 day\").alias(\"7-day windows\")\n",
    "    )\\\n",
    "    .agg(avg(\"Amount\").alias(\"avg\"))\\\n",
    "    .orderBy(\"Country\", \"7-day windows\")\\\n",
    "    .show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating on Group Sets\n",
    "Now I want to drill down and analyse the biggest amounts on levels starting on country level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|        Country|Total Amount|\n",
      "+---------------+------------+\n",
      "|      Australia|      2090.3|\n",
      "|        Austria|        51.0|\n",
      "|        Bahrain|        25.5|\n",
      "|        Belgium|      599.25|\n",
      "|         Brazil|       175.2|\n",
      "|         Canada|       12.75|\n",
      "|Channel Islands|       624.0|\n",
      "|         Cyprus|       382.5|\n",
      "|           EIRE|     8524.35|\n",
      "|        Finland|       38.25|\n",
      "+---------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"StockCode = 22423 or StockCode = 22086\")\\\n",
    "    .groupBy(\"Country\")\\\n",
    "    .agg(sum(\"Amount\").alias(\"sum_amount\"))\\\n",
    "    .select(\"Country\", round(\"sum_amount\", 2).alias(\"Total Amount\"))\\\n",
    "    .orderBy(\"Country\", desc(\"Total Amount\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go further deeper on StockCode level, I just need to  add this column to the grouping key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------------+\n",
      "|        Country|StockCode|Total Amount|\n",
      "+---------------+---------+------------+\n",
      "|      Australia|    22423|      1978.2|\n",
      "|      Australia|    22086|       112.1|\n",
      "|        Austria|    22423|        51.0|\n",
      "|        Bahrain|    22423|        25.5|\n",
      "|        Belgium|    22423|      599.25|\n",
      "|         Brazil|    22423|       175.2|\n",
      "|         Canada|    22423|       12.75|\n",
      "|Channel Islands|    22423|       517.8|\n",
      "|Channel Islands|    22086|       106.2|\n",
      "|         Cyprus|    22423|       382.5|\n",
      "+---------------+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"StockCode = 22423 or StockCode = 22086\")\\\n",
    "    .groupBy(\"Country\", \"StockCode\")\\\n",
    "    .agg(sum(\"Amount\").alias(\"sum_amount\"))\\\n",
    "    .select(\"Country\", \"StockCode\", round(\"sum_amount\", 2).alias(\"Total Amount\"))\\\n",
    "    .orderBy(\"Country\", desc(\"Total Amount\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding more and more columns, I'll get more granular figures, but I will see only the figures on the most detailed level. What should I do to get the sums on all higher levels as well?\n",
    "### Rollup\n",
    "All I need to do is just rolling up the detaild sums up to the top level. Actually I just need to change one word in the code, replacing `groupBy()` by `rollup()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+\n",
      "|  Country|StockCode|Total Amount|\n",
      "+---------+---------+------------+\n",
      "|     null|     null|   228554.13|\n",
      "|Australia|     null|      2090.3|\n",
      "|Australia|    22423|      1978.2|\n",
      "|Australia|    22086|       112.1|\n",
      "|  Austria|     null|        51.0|\n",
      "|  Austria|    22423|        51.0|\n",
      "|  Bahrain|    22423|        25.5|\n",
      "|  Bahrain|     null|        25.5|\n",
      "|  Belgium|     null|      599.25|\n",
      "|  Belgium|    22423|      599.25|\n",
      "+---------+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"StockCode = 22423 or StockCode = 22086\")\\\n",
    "    .rollup(\"Country\", \"StockCode\")\\\n",
    "    .agg(sum(\"Amount\").alias(\"sum_amount\"))\\\n",
    "    .select(\"Country\", \"StockCode\", round(\"sum_amount\", 2).alias(\"Total Amount\"))\\\n",
    "    .orderBy(\"Country\", desc(\"Total Amount\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cube\n",
    "Rolling up aggregations is strongly hierarchical. Now I have the total by country and the sub-total per StockCode per country. But I need the total per StockCode across all countries as well. More generla, I( want to slice and dice the data along each dimension independently from other dimensions, like I can do in relational star schemas.\n",
    "\n",
    "Again, there is only one little piece to tbe changed in the code: using `cube()` instead of `rollup()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+\n",
      "|  Country|StockCode|Total Amount|\n",
      "+---------+---------+------------+\n",
      "|     null|     null|    10436.15|\n",
      "|     null|    22423|    10235.55|\n",
      "|     null|    22086|       200.6|\n",
      "|Australia|     null|      2090.3|\n",
      "|Australia|    22423|      1978.2|\n",
      "|Australia|    22086|       112.1|\n",
      "|  Germany|     null|     8345.85|\n",
      "|  Germany|    22423|     8257.35|\n",
      "|  Germany|    22086|        88.5|\n",
      "+---------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"StockCode = 22423 or StockCode = 22086\")\\\n",
    "    .where(\"Country = 'Australia' or Country = 'Germany'\")\\\n",
    "    .cube(\"Country\", \"StockCode\")\\\n",
    "    .agg(sum(\"Amount\").alias(\"sum_amount\"))\\\n",
    "    .select(\"Country\", \"StockCode\", round(\"sum_amount\", 2).alias(\"Total Amount\"))\\\n",
    "    .orderBy(\"Country\", desc(\"Total Amount\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the second row shows the total for StockCode 22423 across all countries (Country is null ) whereas the fourth row shows the total across StockCodes (StockCode is null) for Australia.\n",
    "\n",
    "Another nice feature is `grouping_id()` which is a special aggregation function. Starting with 0 at the lowest level, the group_id provides the aggregation level in ascending order so I could easily filter on specific detail levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+-----+\n",
      "|  Country|StockCode|Total Amount|Level|\n",
      "+---------+---------+------------+-----+\n",
      "|     null|     null|    10436.15|    3|\n",
      "|     null|    22423|    10235.55|    2|\n",
      "|     null|    22086|       200.6|    2|\n",
      "|Australia|     null|      2090.3|    1|\n",
      "|Australia|    22423|      1978.2|    0|\n",
      "|Australia|    22086|       112.1|    0|\n",
      "|  Germany|     null|     8345.85|    1|\n",
      "|  Germany|    22423|     8257.35|    0|\n",
      "|  Germany|    22086|        88.5|    0|\n",
      "+---------+---------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"StockCode = 22423 or StockCode = 22086\")\\\n",
    "    .where(\"Country = 'Australia' or Country = 'Germany'\")\\\n",
    "    .cube(\"Country\", \"StockCode\")\\\n",
    "    .agg(sum(\"Amount\").alias(\"sum_amount\"), grouping_id().alias(\"Level\"))\\\n",
    "    .select(\"Country\", \"StockCode\", round(\"sum_amount\", 2).alias(\"Total Amount\"), \"Level\")\\\n",
    "    .orderBy(\"Country\", desc(\"Total Amount\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my example the aggregation hierarchy looks like this:\n",
    "\n",
    "3: overall total\n",
    "\n",
    "2: StockCode total\n",
    "\n",
    "1: Country total\n",
    "\n",
    "0: StockCode & Country total\n",
    "\n",
    "Ok, why has StockCode a higher aggregation level than Country? The answer is, they are numbered according to their order in `cube(\"Country\", \"StockCode\")`, so Country comes first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I want to analyse the data for two countries, Germany and Australia, across all StockCode, which can be quite a lot, and I want to compare the German with the Australian figures. Therefore I want to have the totaly by country lined-up in columns. By calling the `pivot()` function, I can flip the country totals from rows to columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+\n",
      "|StockCode|Australia|           Germany|\n",
      "+---------+---------+------------------+\n",
      "|    10002|     null|              0.85|\n",
      "|    10125|     null|              84.8|\n",
      "|    10135|     null|             212.0|\n",
      "|    11001|     null|             54.08|\n",
      "|    15034|     null|              3.36|\n",
      "|    15036|    432.0| 853.3200000000002|\n",
      "|    15039|     null|              0.85|\n",
      "|   15044A|     null|              88.5|\n",
      "|   15044B|     null|              35.4|\n",
      "|   15044D|     null|53.099999999999994|\n",
      "|  15056BL|    17.85|           1100.25|\n",
      "|   15056N|     null|            695.65|\n",
      "|   15056P|     null| 564.7500000000001|\n",
      "|   15058A|     null|              15.9|\n",
      "|   15058B|     null|              15.9|\n",
      "|   15058C|     null|              39.5|\n",
      "|   15060B|     null|              90.0|\n",
      "|    16008|     null|              8.64|\n",
      "|    16011|     null|             20.16|\n",
      "+---------+---------+------------------+\n",
      "only showing top 19 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"Country = 'Australia' or Country = 'Germany'\")\\\n",
    "    .groupBy(\"StockCode\")\\\n",
    "    .pivot(\"Country\")\\\n",
    "    .sum(\"Amount\")\\\n",
    "    .orderBy(\"StockCode\")\\\n",
    "    .show(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Statistical Functions on DataFrames\n",
    "`DataFrame` objects have also some statistical methods. In contrast to the aggregation functions, they always apply on the entire DataFrame data set and cannot be applied on data groups whithin a DataFrame.\n",
    "* **approxQuantile(col, probabilities, relativeError)**\n",
    "* **corr()**\n",
    "* **count()**\n",
    "* **cov()**\n",
    "* **crosstab()**\n",
    "* **describe()** alias **summary()**\n",
    "\n",
    "The `describe()` method is quite interesting because it computes basic statistics (count, mean, stddev, min, and max) for numeric and string columns eventhough ean, stddev, min, and max are not available as `DataFrame` object methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            Amount|\n",
      "+-------+------------------+\n",
      "|  count|            541909|\n",
      "|   mean|17.987794869617858|\n",
      "| stddev|378.81082350632494|\n",
      "|    min|         -168469.6|\n",
      "|    max|          168469.6|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf.describe(\"Amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameStatFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `DataFrame` object has an `na` property referencing a `DataFrameStatFunctions` object which provides statisical object methods. Some of them are just aliases of `DataFrame` object methods, e.g.:\n",
    "\n",
    "* df.**corr()** alias df.**na.corr()** - calculates the Pearson Correlation Coefficient of two columns of a DataFrame as a double value\n",
    "* df.**cov()** alias df.**na.cov()** - calculate the sample covariance for the given columns, specified by their names, as a double value\n",
    "* df.**crosstab()** alias df.**na.crosstab()** - computes a pair-wise frequency table of the given columns. Also known as a contingency table\n",
    "\n",
    "I admit, I've no clue about the reason for this duplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}